{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfb9d07-1157-432a-9fcc-1018dfa5635f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f935f4b3-7790-434e-8411-2fe8c33fe736",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d729c81b-91be-4cf6-b428-1f171cb80810",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "users = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"first_name\": \"Corrie\",\n",
    "        \"last_name\": \"Van den Oord\",        \n",
    "        \"email\": \"cvandenoor@etsy.com\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1000.55,\n",
    "        \"customer_from\": datetime.date(2021,1,15),\n",
    "        \"last_updated_ts\": datetime.datetime(2021,2,10,1,15,0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"John\",\n",
    "        \"last_name\": \"Cena\",        \n",
    "        \"email\": \"john@cena.com\",        \n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 900.0,\n",
    "        \"customer_from\": datetime.date(2022,5,15),\n",
    "        \"last_updated_ts\": datetime.datetime(2024,3,15,1,16,0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"James\",\n",
    "        \"last_name\": \"Bond\",        \n",
    "        \"email\": \"james@bond.com\",        \n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": 750.60,\n",
    "        \"customer_from\": datetime.date(2023,1,12),\n",
    "        \"last_updated_ts\": datetime.datetime(2018,5,5,5,17,2)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"James\",\n",
    "        \"last_name\": \"Bond\",        \n",
    "        \"email\": \"james@bond.com\",        \n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": 750.60,\n",
    "        \"customer_from\": datetime.date(2023,1,12),\n",
    "        \"last_updated_ts\": datetime.datetime(2018,5,5,5,17,2)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Robert\",\n",
    "        \"last_name\": \"Dowrey\",        \n",
    "        \"email\": \"robert@dowrey.com\",        \n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 500.50,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2019,4,3,8,14,8)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Robert\",\n",
    "        \"last_name\": \"Dowrey\",        \n",
    "        \"email\": \"robert@dowrey.com\",        \n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 500.50,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2019,4,3,8,14,8)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": \"Chris\",\n",
    "        \"last_name\": \"Hemmsworth\",        \n",
    "        \"email\": \"chris@hemmsworth.com\",        \n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2019,4,3,8,14,8)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": \"Chris\",\n",
    "        \"last_name\": \"Hemmsworth\",        \n",
    "        \"email\": \"chris@hemmsworth.com\",        \n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": 500.0,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2018,3,4,6,12,6)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e335b2dc-2ce3-45b0-aebc-68c38ace024d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "userdf = spark.createDataFrame(pd.DataFrame(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f78c1c53-a50a-48d3-a1f9-8c2136539665",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|id |first_name|last_name   |email               |is_customer|amount_paid|customer_from|last_updated_ts    |\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1  |Corrie    |Van den Oord|cvandenoor@etsy.com |true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|\n|2  |John      |Cena        |john@cena.com       |true       |900.0      |2022-05-15   |2024-03-15 01:16:00|\n|3  |James     |Bond        |james@bond.com      |false      |750.6      |2023-01-12   |2018-05-05 05:17:02|\n|3  |James     |Bond        |james@bond.com      |false      |750.6      |2023-01-12   |2018-05-05 05:17:02|\n|4  |Robert    |Dowrey      |robert@dowrey.com   |true       |500.5      |NULL         |2019-04-03 08:14:08|\n|4  |Robert    |Dowrey      |robert@dowrey.com   |true       |500.5      |NULL         |2019-04-03 08:14:08|\n|5  |Chris     |Hemmsworth  |chris@hemmsworth.com|false      |NaN        |NULL         |2019-04-03 08:14:08|\n|5  |Chris     |Hemmsworth  |chris@hemmsworth.com|false      |500.0      |NULL         |2018-03-04 06:12:06|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8470e2c-9929-4cf7-a9c5-d9d1f16ab36b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc5b81e-8e74-4adc-8464-fcfce2aa1e47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method distinct in module pyspark.sql.dataframe:\n\ndistinct() -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with distinct records.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame(\n    ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n    \n    Return the number of distinct rows in the :class:`DataFrame`\n    \n    >>> df.distinct().count()\n    2\n\n"
     ]
    }
   ],
   "source": [
    "help(userdf.distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398bed66-7f0f-4bed-9495-442cc018d9cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdf.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0a2a65-8f62-42b7-8986-b82b1c67c192",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n\ndropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n    \n    .. versionadded:: 1.4\n\n"
     ]
    }
   ],
   "source": [
    "help(userdf.drop_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764f664f-d4ab-43ce-a6b8-f65335c719a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n\ndropDuplicates(subset: Optional[List[str]] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Return a new :class:`DataFrame` with duplicate rows removed,\n    optionally only considering certain columns.\n    \n    For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n    :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n    duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n    be and the system will accordingly limit the state. In addition, data older than\n    watermark will be dropped to avoid any possibility of duplicates.\n    \n    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    subset : List of column names, optional\n        List of columns to use for duplicate comparison (default All columns).\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame without duplicates.\n    \n    Examples\n    --------\n    >>> from pyspark.sql import Row\n    >>> df = spark.createDataFrame([\n    ...     Row(name='Alice', age=5, height=80),\n    ...     Row(name='Alice', age=5, height=80),\n    ...     Row(name='Alice', age=10, height=80)\n    ... ])\n    \n    Deduplicate the same rows.\n    \n    >>> df.dropDuplicates().show()\n    +-----+---+------+\n    | name|age|height|\n    +-----+---+------+\n    |Alice|  5|    80|\n    |Alice| 10|    80|\n    +-----+---+------+\n    \n    Deduplicate values on 'name' and 'height' columns.\n    \n    >>> df.dropDuplicates(['name', 'height']).show()\n    +-----+---+------+\n    | name|age|height|\n    +-----+---+------+\n    |Alice|  5|    80|\n    +-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "help(userdf.dropDuplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36c1edd3-1558-4b39-9f4c-ba45edc31bb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- We can also drop duplicates based on certain columns\n",
    "- The below operation will fail as the function expects sequence type object such as list or array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3566db-52f4-4ed8-a789-4208881c2e82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3653095131636539>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43muserdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropDuplicates\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4074\u001B[0m, in \u001B[0;36mDataFrame.dropDuplicates\u001B[0;34m(self, subset)\u001B[0m\n",
       "\u001B[1;32m   4019\u001B[0m \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` with duplicate rows removed,\u001B[39;00m\n",
       "\u001B[1;32m   4020\u001B[0m \u001B[38;5;124;03moptionally only considering certain columns.\u001B[39;00m\n",
       "\u001B[1;32m   4021\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   4071\u001B[0m \u001B[38;5;124;03m+-----+---+------+\u001B[39;00m\n",
       "\u001B[1;32m   4072\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   4073\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(subset, Iterable) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(subset, \u001B[38;5;28mstr\u001B[39m)):\n",
       "\u001B[0;32m-> 4074\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   4075\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_LIST_OR_TUPLE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   4076\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(subset)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   4077\u001B[0m     )\n",
       "\u001B[1;32m   4079\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   4080\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mdropDuplicates()\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got str."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\nFile \u001B[0;32m<command-3653095131636539>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43muserdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropDuplicates\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4074\u001B[0m, in \u001B[0;36mDataFrame.dropDuplicates\u001B[0;34m(self, subset)\u001B[0m\n\u001B[1;32m   4019\u001B[0m \u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` with duplicate rows removed,\u001B[39;00m\n\u001B[1;32m   4020\u001B[0m \u001B[38;5;124;03moptionally only considering certain columns.\u001B[39;00m\n\u001B[1;32m   4021\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4071\u001B[0m \u001B[38;5;124;03m+-----+---+------+\u001B[39;00m\n\u001B[1;32m   4072\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4073\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(subset, Iterable) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(subset, \u001B[38;5;28mstr\u001B[39m)):\n\u001B[0;32m-> 4074\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   4075\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_LIST_OR_TUPLE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   4076\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(subset)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   4077\u001B[0m     )\n\u001B[1;32m   4079\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   4080\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mdropDuplicates()\n\n\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got str.",
       "errorSummary": "<span class='ansi-red-fg'>PySparkTypeError</span>: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got str.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "userdf.dropDuplicates('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b42bdaa-e242-4d11-b6e8-8e68a8339fc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|id |first_name|last_name   |email               |is_customer|amount_paid|customer_from|last_updated_ts    |\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|1  |Corrie    |Van den Oord|cvandenoor@etsy.com |true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|\n|2  |John      |Cena        |john@cena.com       |true       |900.0      |2022-05-15   |2024-03-15 01:16:00|\n|3  |James     |Bond        |james@bond.com      |false      |750.6      |2023-01-12   |2018-05-05 05:17:02|\n|3  |James     |Bond        |james@bond.com      |false      |750.6      |2023-01-12   |2018-05-05 05:17:02|\n|4  |Robert    |Dowrey      |robert@dowrey.com   |true       |500.5      |NULL         |2019-04-03 08:14:08|\n|4  |Robert    |Dowrey      |robert@dowrey.com   |true       |500.5      |NULL         |2019-04-03 08:14:08|\n|5  |Chris     |Hemmsworth  |chris@hemmsworth.com|false      |NaN        |NULL         |2019-04-03 08:14:08|\n|5  |Chris     |Hemmsworth  |chris@hemmsworth.com|false      |500.0      |NULL         |2018-03-04 06:12:06|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cdd0193-b9f9-4c43-9aa1-af67d7bd7559",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord| cvandenoor@etsy.com|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|      John|        Cena|       john@cena.com|       true|      900.0|   2022-05-15|2024-03-15 01:16:00|\n|  3|     James|        Bond|      james@bond.com|      false|      750.6|   2023-01-12|2018-05-05 05:17:02|\n|  4|    Robert|      Dowrey|   robert@dowrey.com|       true|      500.5|         NULL|2019-04-03 08:14:08|\n|  5|     Chris|  Hemmsworth|chris@hemmsworth.com|      false|        NaN|         NULL|2019-04-03 08:14:08|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf.dropDuplicates(['id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2a183a-4fe8-4484-9521-1a41e7c5ac55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord| cvandenoor@etsy.com|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|      John|        Cena|       john@cena.com|       true|      900.0|   2022-05-15|2024-03-15 01:16:00|\n|  3|     James|        Bond|      james@bond.com|      false|      750.6|   2023-01-12|2018-05-05 05:17:02|\n|  4|    Robert|      Dowrey|   robert@dowrey.com|       true|      500.5|         NULL|2019-04-03 08:14:08|\n|  5|     Chris|  Hemmsworth|chris@hemmsworth.com|      false|      500.0|         NULL|2018-03-04 06:12:06|\n|  5|     Chris|  Hemmsworth|chris@hemmsworth.com|      false|        NaN|         NULL|2019-04-03 08:14:08|\n+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf.dropDuplicates(['id','amount_paid']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0d044f-2847-49f9-b2e6-30a757f7041a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07 - Dropping duplicate records from Spark Data Frames",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

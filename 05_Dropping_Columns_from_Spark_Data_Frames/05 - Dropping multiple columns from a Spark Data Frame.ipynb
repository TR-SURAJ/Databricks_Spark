{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae87103f-1bc8-4fb4-8281-e47b2eba6a71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+--------------------+----------+---+-----------+------------+-------------------+--------------------------------+\n|amount_paid|courses|customer_from|email               |first_name|id |is_customer|last_name   |last_updated_ts    |phone_numbers                   |\n+-----------+-------+-------------+--------------------+----------+---+-----------+------------+-------------------+--------------------------------+\n|1000.55    |[1, 2] |2021-01-15   |cvandenoor@etsy.com |Corrie    |1  |true       |Van den Oord|2021-02-10 01:15:00|{+91 8645879087, +91 9878673289}|\n|900.0      |[3]    |2022-05-15   |john@cena.com       |John      |2  |true       |Cena        |2024-03-15 01:16:00|{+91 9886879087, +91 9134673289}|\n|750.6      |[2, 3] |2023-01-12   |james@bond.com      |James     |3  |false      |Bond        |2018-05-05 05:17:02|{+91 3245879087, +91 9854673289}|\n|NULL       |[]     |NULL         |robert@dowrey.com   |Robert    |4  |false      |Dowrey      |2019-04-03 08:14:08|NULL                            |\n|NULL       |[]     |NULL         |chris@hemmsworth.com|Chris     |5  |false      |Hemmsworth  |2019-04-03 08:14:08|{+91 9085879087, }              |\n+-----------+-------+-------------+--------------------+----------+---+-----------+------------+-------------------+--------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/surajthallapalli@outlook.com/05 Dropping Columns from Spark Data Frames/02 - Creating Spark Data Frame for Dropping Columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ebee37b-654f-43d8-a7a2-f1a86ca7f0fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- amount_paid: double (nullable = true)\n |-- courses: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- customer_from: date (nullable = true)\n |-- email: string (nullable = true)\n |-- id: long (nullable = true)\n |-- is_customer: boolean (nullable = true)\n |-- last_updated_ts: timestamp (nullable = true)\n |-- phone_numbers: struct (nullable = true)\n |    |-- mobile: string (nullable = true)\n |    |-- home: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "userdf.drop('first_name','last_name').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97cfb16-431f-4a59-8f80-56e91b16981e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+--------------------+---+-----------+-------------------+--------------------+\n|amount_paid|courses|customer_from|               email| id|is_customer|    last_updated_ts|       phone_numbers|\n+-----------+-------+-------------+--------------------+---+-----------+-------------------+--------------------+\n|    1000.55| [1, 2]|   2021-01-15| cvandenoor@etsy.com|  1|       true|2021-02-10 01:15:00|{+91 8645879087, ...|\n|      900.0|    [3]|   2022-05-15|       john@cena.com|  2|       true|2024-03-15 01:16:00|{+91 9886879087, ...|\n|      750.6| [2, 3]|   2023-01-12|      james@bond.com|  3|      false|2018-05-05 05:17:02|{+91 3245879087, ...|\n|       NULL|     []|         NULL|   robert@dowrey.com|  4|      false|2019-04-03 08:14:08|                NULL|\n|       NULL|     []|         NULL|chris@hemmsworth.com|  5|      false|2019-04-03 08:14:08|  {+91 9085879087, }|\n+-----------+-------+-------------+--------------------+---+-----------+-------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf.drop('first_name','last_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d0f4a2-13b7-4cff-8588-279d41899bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd2bcec-08a4-435b-952c-8ae293b1ba26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+--------------------+---+-----------+------------+-------------------+--------------------+\n|amount_paid|courses|customer_from|               email| id|is_customer|   last_name|    last_updated_ts|       phone_numbers|\n+-----------+-------+-------------+--------------------+---+-----------+------------+-------------------+--------------------+\n|    1000.55| [1, 2]|   2021-01-15| cvandenoor@etsy.com|  1|       true|Van den Oord|2021-02-10 01:15:00|{+91 8645879087, ...|\n|      900.0|    [3]|   2022-05-15|       john@cena.com|  2|       true|        Cena|2024-03-15 01:16:00|{+91 9886879087, ...|\n|      750.6| [2, 3]|   2023-01-12|      james@bond.com|  3|      false|        Bond|2018-05-05 05:17:02|{+91 3245879087, ...|\n|       NULL|     []|         NULL|   robert@dowrey.com|  4|      false|      Dowrey|2019-04-03 08:14:08|                NULL|\n|       NULL|     []|         NULL|chris@hemmsworth.com|  5|      false|  Hemmsworth|2019-04-03 08:14:08|  {+91 9085879087, }|\n+-----------+-------+-------------+--------------------+---+-----------+------------+-------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf.drop(col('first_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b7ec361-21e0-493c-84b7-822fdacfe435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+--------------------+-----------+------------+-------------------+--------------------+\n|amount_paid|courses|customer_from|               email|is_customer|   last_name|    last_updated_ts|       phone_numbers|\n+-----------+-------+-------------+--------------------+-----------+------------+-------------------+--------------------+\n|    1000.55| [1, 2]|   2021-01-15| cvandenoor@etsy.com|       true|Van den Oord|2021-02-10 01:15:00|{+91 8645879087, ...|\n|      900.0|    [3]|   2022-05-15|       john@cena.com|       true|        Cena|2024-03-15 01:16:00|{+91 9886879087, ...|\n|      750.6| [2, 3]|   2023-01-12|      james@bond.com|      false|        Bond|2018-05-05 05:17:02|{+91 3245879087, ...|\n|       NULL|     []|         NULL|   robert@dowrey.com|      false|      Dowrey|2019-04-03 08:14:08|                NULL|\n|       NULL|     []|         NULL|chris@hemmsworth.com|      false|  Hemmsworth|2019-04-03 08:14:08|  {+91 9085879087, }|\n+-----------+-------+-------------+--------------------+-----------+------------+-------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf.drop(col('first_name'), col('id')).show() ## ???????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b837313a-d31e-4e36-b2fb-8b1f7742abdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop in module pyspark.sql.dataframe:\n\ndrop(*cols: 'ColumnOrName') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` without specified columns.\n    This is a no-op if the schema doesn't contain the given column name(s).\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    cols: str or :class:`Column`\n        a name of the column, or the :class:`Column` to drop\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame without given columns.\n    \n    Notes\n    -----\n    When an input is a column name, it is treated literally without further interpretation.\n    Otherwise, will try to match the equivalent expression.\n    So that dropping column by its name `drop(colName)` has different semantic with directly\n    dropping the column `drop(col(colName))`.\n    \n    Examples\n    --------\n    >>> from pyspark.sql import Row\n    >>> from pyspark.sql.functions import col, lit\n    >>> df = spark.createDataFrame(\n    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n    >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n    \n    >>> df.drop('age').show()\n    +-----+\n    | name|\n    +-----+\n    |  Tom|\n    |Alice|\n    |  Bob|\n    +-----+\n    >>> df.drop(df.age).show()\n    +-----+\n    | name|\n    +-----+\n    |  Tom|\n    |Alice|\n    |  Bob|\n    +-----+\n    \n    Drop the column that joined both DataFrames on.\n    \n    >>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n    +---+------+\n    |age|height|\n    +---+------+\n    | 14|    80|\n    | 16|    85|\n    +---+------+\n    \n    >>> df3 = df.join(df2)\n    >>> df3.show()\n    +---+-----+------+----+\n    |age| name|height|name|\n    +---+-----+------+----+\n    | 14|  Tom|    80| Tom|\n    | 14|  Tom|    85| Bob|\n    | 23|Alice|    80| Tom|\n    | 23|Alice|    85| Bob|\n    | 16|  Bob|    80| Tom|\n    | 16|  Bob|    85| Bob|\n    +---+-----+------+----+\n    \n    Drop two column by the same name.\n    \n    >>> df3.drop(\"name\").show()\n    +---+------+\n    |age|height|\n    +---+------+\n    | 14|    80|\n    | 14|    85|\n    | 23|    80|\n    | 23|    85|\n    | 16|    80|\n    | 16|    85|\n    +---+------+\n    \n    Can not drop col('name') due to ambiguous reference.\n    \n    >>> df3.drop(col(\"name\")).show()\n    Traceback (most recent call last):\n    ...\n    pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n    \n    >>> df4 = df.withColumn(\"a.b.c\", lit(1))\n    >>> df4.show()\n    +---+-----+-----+\n    |age| name|a.b.c|\n    +---+-----+-----+\n    | 14|  Tom|    1|\n    | 23|Alice|    1|\n    | 16|  Bob|    1|\n    +---+-----+-----+\n    \n    >>> df4.drop(\"a.b.c\").show()\n    +---+-----+\n    |age| name|\n    +---+-----+\n    | 14|  Tom|\n    | 23|Alice|\n    | 16|  Bob|\n    +---+-----+\n    \n    Can not find a column matching the expression \"a.b.c\".\n    \n    >>> df4.drop(col(\"a.b.c\")).show()\n    +---+-----+-----+\n    |age| name|a.b.c|\n    +---+-----+-----+\n    | 14|  Tom|    1|\n    | 23|Alice|    1|\n    | 16|  Bob|    1|\n    +---+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "help(userdf.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b16e5b4-4ba7-4d4f-a3e5-3f8947d61aa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05 - Dropping multiple columns from a Spark Data Frame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

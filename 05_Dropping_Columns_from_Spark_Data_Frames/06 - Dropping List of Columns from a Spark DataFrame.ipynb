{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e15e789-d37e-4cc7-b414-4e7feda5b0d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+--------------------+----------+---+-----------+------------+-------------------+--------------------------------+\n|amount_paid|courses|customer_from|email               |first_name|id |is_customer|last_name   |last_updated_ts    |phone_numbers                   |\n+-----------+-------+-------------+--------------------+----------+---+-----------+------------+-------------------+--------------------------------+\n|1000.55    |[1, 2] |2021-01-15   |cvandenoor@etsy.com |Corrie    |1  |true       |Van den Oord|2021-02-10 01:15:00|{+91 8645879087, +91 9878673289}|\n|900.0      |[3]    |2022-05-15   |john@cena.com       |John      |2  |true       |Cena        |2024-03-15 01:16:00|{+91 9886879087, +91 9134673289}|\n|750.6      |[2, 3] |2023-01-12   |james@bond.com      |James     |3  |false      |Bond        |2018-05-05 05:17:02|{+91 3245879087, +91 9854673289}|\n|NULL       |[]     |NULL         |robert@dowrey.com   |Robert    |4  |false      |Dowrey      |2019-04-03 08:14:08|NULL                            |\n|NULL       |[]     |NULL         |chris@hemmsworth.com|Chris     |5  |false      |Hemmsworth  |2019-04-03 08:14:08|{+91 9085879087, }              |\n+-----------+-------+-------------+--------------------+----------+---+-----------+------------+-------------------+--------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/surajthallapalli@outlook.com/05 Dropping Columns from Spark Data Frames/02 - Creating Spark Data Frame for Dropping Columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f268d2ec-d7df-4962-a2de-a5005192557d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9bbe5c-f8fd-4bf6-a37a-d781ac57bb13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pii = ['first_name','last_name','email','phone_numbers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b8e29d-aec5-4342-8899-2a403ded11a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3653095131636520>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m userdf_nopii \u001B[38;5;241m=\u001B[39m \u001B[43muserdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpii\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5295\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   5293\u001B[0m         java_columns\u001B[38;5;241m.\u001B[39mappend(c\u001B[38;5;241m.\u001B[39m_jc)\n",
       "\u001B[1;32m   5294\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 5295\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   5296\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN_OR_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   5297\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(c)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   5298\u001B[0m         )\n",
       "\u001B[1;32m   5300\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\n",
       "\u001B[1;32m   5301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(java_columns) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\nFile \u001B[0;32m<command-3653095131636520>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m userdf_nopii \u001B[38;5;241m=\u001B[39m \u001B[43muserdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpii\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5295\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   5293\u001B[0m         java_columns\u001B[38;5;241m.\u001B[39mappend(c\u001B[38;5;241m.\u001B[39m_jc)\n\u001B[1;32m   5294\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 5295\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   5296\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN_OR_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5297\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(c)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   5298\u001B[0m         )\n\u001B[1;32m   5300\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\n\u001B[1;32m   5301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(java_columns) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\n\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.",
       "errorSummary": "<span class='ansi-red-fg'>PySparkTypeError</span>: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This will fail. We need to convert list to varying arguments to get it working\n",
    "userdf_nopii = userdf.drop(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae245d2f-9b79-45a9-b244-2017ffa2522f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "userdf_nopii = userdf.drop(*pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588929a8-514d-408e-8379-48d33f97ab53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+---+-----------+-------------------+\n|amount_paid|courses|customer_from| id|is_customer|    last_updated_ts|\n+-----------+-------+-------------+---+-----------+-------------------+\n|    1000.55| [1, 2]|   2021-01-15|  1|       true|2021-02-10 01:15:00|\n|      900.0|    [3]|   2022-05-15|  2|       true|2024-03-15 01:16:00|\n|      750.6| [2, 3]|   2023-01-12|  3|      false|2018-05-05 05:17:02|\n|       NULL|     []|         NULL|  4|      false|2019-04-03 08:14:08|\n|       NULL|     []|         NULL|  5|      false|2019-04-03 08:14:08|\n+-----------+-------+-------------+---+-----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf_nopii.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07a52a5-5d8a-40ae-b1ec-e3c7ee9398a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e12f04-a3f1-4668-9a3b-b0ce95dc2c22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If any col which doesnt exist in dataframe it will be ignored on drop\n",
    "pii = ['first_name','last_name','email','phone_numbers','abc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be05173-2619-4a17-bc20-bf40dc577fbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+---+-----------+-------------------+\n|amount_paid|courses|customer_from| id|is_customer|    last_updated_ts|\n+-----------+-------+-------------+---+-----------+-------------------+\n|    1000.55| [1, 2]|   2021-01-15|  1|       true|2021-02-10 01:15:00|\n|      900.0|    [3]|   2022-05-15|  2|       true|2024-03-15 01:16:00|\n|      750.6| [2, 3]|   2023-01-12|  3|      false|2018-05-05 05:17:02|\n|       NULL|     []|         NULL|  4|      false|2019-04-03 08:14:08|\n|       NULL|     []|         NULL|  5|      false|2019-04-03 08:14:08|\n+-----------+-------+-------------+---+-----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "userdf.drop(*pii).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff3fbd0b-e0ca-41cf-ae77-4ecb1606a8b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06 - Dropping List of Columns from a Spark DataFrame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

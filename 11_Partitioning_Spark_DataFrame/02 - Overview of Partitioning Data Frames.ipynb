{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1fea63-7f79-43e1-9e5d-1fb32d23c6b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498b6dec-f4dc-4779-b6e7-8f5f051137c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x7f75b0f5a7a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd04bde8-6341-40e9-a567-8a754f72f0af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method partitionBy in module pyspark.sql.readwriter:\n\npartitionBy(*cols: Union[str, List[str]]) -> 'DataFrameWriter' method of pyspark.sql.readwriter.DataFrameWriter instance\n    Partitions the output by the given columns on the file system.\n    \n    If specified, the output is laid out on the file system similar\n    to Hive's partitioning scheme.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    cols : str or list\n        name of columns\n    \n    Examples\n    --------\n    Write a DataFrame into a Parquet file in a partitioned manner, and read it back.\n    \n    >>> import tempfile\n    >>> import os\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a Parquet file in a partitioned manner.\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}, {\"age\": 120, \"name\": \"Ruifeng Zheng\"}]\n    ...     ).write.partitionBy(\"name\").mode(\"overwrite\").format(\"parquet\").save(d)\n    ...\n    ...     # Read the Parquet file as a DataFrame.\n    ...     spark.read.parquet(d).sort(\"age\").show()\n    ...\n    ...     # Read one partition as a DataFrame.\n    ...     spark.read.parquet(f\"{d}{os.path.sep}name=Hyukjin Kwon\").show()\n    +---+-------------+\n    |age|         name|\n    +---+-------------+\n    |100| Hyukjin Kwon|\n    |120|Ruifeng Zheng|\n    +---+-------------+\n    +---+\n    |age|\n    +---+\n    |100|\n    +---+\n\n"
     ]
    }
   ],
   "source": [
    "help(df.write.partitionBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56034ba9-ea19-4c2a-8b64-b738531e9e35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbb9ee3-0e0c-496a-9aee-0dbfec2f6122",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method json in module pyspark.sql.readwriter:\n\njson(path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in JSON format\n    (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n    specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a JSON file and read it back.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a JSON file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n    ...     ).write.json(d, mode=\"overwrite\")\n    ...\n    ...     # Read the JSON file as a DataFrame.\n    ...     spark.read.format(\"json\").load(d).show()\n    +---+------------+\n    |age|        name|\n    +---+------------+\n    |100|Hyukjin Kwon|\n    +---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#json does not have keyword argument related to partitioning\n",
    "help(df.write.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6b546f-477f-4927-876f-d6665739b8c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parquet in module pyspark.sql.readwriter:\n\nparquet(path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    partitionBy : str or list, optional\n        names of partitioning columns\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a Parquet file and read it back.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a Parquet file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n    ...     ).write.parquet(d, mode=\"overwrite\")\n    ...\n    ...     # Read the Parquet file as a DataFrame.\n    ...     spark.read.format(\"parquet\").load(d).show()\n    +---+------------+\n    |age|        name|\n    +---+------------+\n    |100|Hyukjin Kwon|\n    +---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#parquet have keyword argument partitionBy\n",
    "help(df.write.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea357a8-0f95-411f-8212-e67af115944d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Overview of Partitioning Data Frames",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "844cc807-2d35-4631-b9f7-1f71cc4040f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- We can read the data from CSV files into Spark Data Frame using multiple approaches\n",
    "- Approach 1: spark.read.csv('path_to_folder')\n",
    "- Approach 2: spark.read.format('csv').load('path_to_folder')\n",
    "- We can explicitly specify the schema as **string** or using **StructType**\n",
    "- We can also read the data which is delimited or seperated by other characters than comma\n",
    "- If the files have header we can create the DataFrame with schema by using options such as **header** and **inferSchema**. It will pick column names from the header while data types will be inferred based on the data\n",
    "- If the files does not have header we can create the DataFrame with schema by passing column names using toDF and by using inferSchema option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1394a83a-412d-469c-9d33-acbdfc3a9c28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Default Behavior\n",
    "# It will delimit the data using comma as seperator\n",
    "# Column names will be system generated\n",
    "# All the fields will be of type strings\n",
    "\n",
    "orders = spark.read.csv('/public/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d307c55-7cc1-4b06-b284-b77c9a02e8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380c2790-7591-4847-9555-b07b492e53a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "115b2119-5d3e-4fd6-9aee-452d991e4614",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06 - Reading Data from CSV files into Spark Data Frame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

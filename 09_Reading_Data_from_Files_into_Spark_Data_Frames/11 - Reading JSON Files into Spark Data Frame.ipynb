{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97ca632-9a59-448e-ac45-69ebe44485df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method json in module pyspark.sql.readwriter:\n\njson(path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads JSON files and returns the results as a :class:`DataFrame`.\n    \n    `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n    \n    If the ``schema`` parameter is not specified, this function goes\n    through the input once to determine the input schema.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str, list or :class:`RDD`\n        string represents path to the JSON dataset, or a list of paths,\n        or RDD of Strings storing JSON objects.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema or\n        a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a JSON file and read it back.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a JSON file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n    ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n    ...\n    ...     # Read the JSON file as a DataFrame.\n    ...     spark.read.json(d).show()\n    +---+------------+\n    |age|        name|\n    +---+------------+\n    |100|Hyukjin Kwon|\n    +---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5a9902-fbb6-4f3f-ac57-ce462cd74afb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema will be inferred by default\n",
    "df = spark.read.json('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d02952f-51f1-4c82-8772-4f0418ae0752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9add64e0-415b-4713-b999-965b84cb03ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a289175c-f1c4-4efb-8561-4c2fbd6013d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74a6b0f-7ff7-4e8c-84de-fd3617207031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['dbfs:/public/retail_db_json/orders/part-r-00000-990f5773-9005-49ba-b670-631286032674']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b6f577-c67c-4403-b1e7-0c1c50fa6de9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('order_customer_id', 'bigint'),\n",
       " ('order_date', 'string'),\n",
       " ('order_id', 'bigint'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee49985-5d15-4fa8-8abf-4ba2a55e6fc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dfc63eb-c798-4bc9-8705-633f7bef50d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "11 - Reading JSON Files into Spark Data Frame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

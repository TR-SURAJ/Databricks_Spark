{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109d6385-0904-417a-9753-a8aa505236d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method schema in module pyspark.sql.readwriter:\n\nschema(schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader' method of pyspark.sql.readwriter.DataFrameReader instance\n    Specifies the input schema.\n    \n    Some data sources (e.g. JSON) can infer the input schema automatically from data.\n    By specifying the schema here, the underlying data source can skip the schema\n    inference step, and thus speed up data loading.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    schema : :class:`pyspark.sql.types.StructType` or str\n        a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n        (For example ``col0 INT, col1 DOUBLE``).\n    \n    Examples\n    --------\n    >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n    <...readwriter.DataFrameReader object ...>\n    \n    Specify the schema with reading a CSV file.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n    root\n     |-- col0: integer (nullable = true)\n     |-- col1: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25531a9c-efd7-40e1-8a2f-23bbcd60f001",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "    order_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f4a5409-cb13-499e-a868-64c9e3dc1bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Mehod 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93984dc-637c-45ba-a9fe-7d3fd0ab7612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(schema).csv('/public/retail_db/orders').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6adacf2-6002-46fa-a352-091a6d358ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef5a56d-2bc1-4617-a489-b8d81d8482fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/public/retail_db/orders', schema = schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c0d57c-4b88-4123-b531-a150fd5e8d11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load in module pyspark.sql.readwriter:\n\nload(path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads data from a data source and returns it as a :class:`DataFrame`.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str or list, optional\n        optional string or a list of string for file-system backed data sources.\n    format : str, optional\n        optional string for format of the data source. Default to 'parquet'.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        optional :class:`pyspark.sql.types.StructType` for the input schema\n        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    **options : dict\n        all other string options\n    \n    Examples\n    --------\n    Load a CSV file with format, schema and options specified.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a CSV file with a header\n    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n    ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n    ...\n    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n    ...     # and 'header' option set to `True`.\n    ...     df = spark.read.load(\n    ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n    ...     df.printSchema()\n    ...     df.show()\n    root\n     |-- age: long (nullable = true)\n     |-- name: string (nullable = true)\n    +---+----+\n    |age|name|\n    +---+----+\n    |100|NULL|\n    +---+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.format('csv').load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e924287f-656c-4d2b-98c8-e1eeef2546f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0a1b8f-b6bb-4613-85fa-c497b5765679",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "494f5198-9fda-4dbb-a4ba-9272faf513d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('order_id', IntegerType()),\n",
    "    StructField('order_date', TimestampType()),\n",
    "    StructField('order_customer_id', IntegerType()),\n",
    "    StructField('order_status', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52a0317-eff2-4139-bcc4-06975d65df27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(schema).csv('/public/retail_db/orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16eb44e-f9d5-4641-b61c-3a1c715571ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/public/retail_db/orders', schema = schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594c62b5-5e71-42ff-8953-588fa779a792",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('order_id', IntegerType(), nullable = False),\n",
    "    StructField('order_date', TimestampType(), nullable = False),\n",
    "    StructField('order_customer_id', IntegerType(), nullable = False),\n",
    "    StructField('order_status', StringType(), nullable = False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e707ff-d839-42df-a59e-690a1d8fcdf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(schema).csv('/public/retail_db/orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876fb061-ce0e-4893-b9af-2b079a1595b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/public/retail_db/orders', schema = schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1bc207-46f2-40e5-8014-d82446186bdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- order_id: integer (nullable = true)\n |-- order_date: timestamp (nullable = true)\n |-- order_customer_id: integer (nullable = true)\n |-- order_status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(schema).csv('/public/retail_db/orders').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c0fdd99-ed41-41b4-b50f-c1995f62b901",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07 - Specifying Schema while reading CSV data into Data Frame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ec1fbe-284a-49d1-861d-2f6d5747d1f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- We can pass the options using different ways while creating the Data Frame\n",
    "  - Using key word arguments as part of APIs. We can use key word arguments as part of load as well as direct API\n",
    "  - spark.read.option\n",
    "  - spark.read.options\n",
    "  - If key in the option is incorrect then the options will be ignored\n",
    "\n",
    "- Depending up on the API based on the file format the options as well as arguments vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d9053f-818c-435b-9ec0-6c68a1f03183",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1e7a2f-7c2d-4158-86a8-52f8c411d864",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.csv(f'/user/{username}/retail_db_pipe/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d088e098-5ef2-4e35-a0b6-a71ee1d5ec8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n|_c0                                          |\n+---------------------------------------------+\n|1|2013-07-25 00:00:00.0|11599|CLOSED         |\n|2|2013-07-25 00:00:00.0|256|PENDING_PAYMENT  |\n|3|2013-07-25 00:00:00.0|12111|COMPLETE       |\n|4|2013-07-25 00:00:00.0|8827|CLOSED          |\n|5|2013-07-25 00:00:00.0|11318|COMPLETE       |\n|6|2013-07-25 00:00:00.0|7130|COMPLETE        |\n|7|2013-07-25 00:00:00.0|4530|COMPLETE        |\n|8|2013-07-25 00:00:00.0|2911|PROCESSING      |\n|9|2013-07-25 00:00:00.0|5657|PENDING_PAYMENT |\n|10|2013-07-25 00:00:00.0|5648|PENDING_PAYMENT|\n|11|2013-07-25 00:00:00.0|918|PAYMENT_REVIEW  |\n|12|2013-07-25 00:00:00.0|1837|CLOSED         |\n|13|2013-07-25 00:00:00.0|9149|PENDING_PAYMENT|\n|14|2013-07-25 00:00:00.0|9842|PROCESSING     |\n|15|2013-07-25 00:00:00.0|2568|COMPLETE       |\n|16|2013-07-25 00:00:00.0|7276|PENDING_PAYMENT|\n|17|2013-07-25 00:00:00.0|2667|COMPLETE       |\n|18|2013-07-25 00:00:00.0|1205|CLOSED         |\n|19|2013-07-25 00:00:00.0|9488|PENDING_PAYMENT|\n|20|2013-07-25 00:00:00.0|9198|PROCESSING     |\n+---------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "orders.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea3d7b3-3c65-4d16-b81b-4ef74e29f4e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.csv(f'/user/{username}/retail_db_pipe/orders', sep = '|', header = None, inferSchema = True).\\\n",
    "         toDF('order_id','order_date','order_customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25fb26f2-489a-42aa-9bc6-6fd5e6ce7b0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96eec475-097c-4323-ae50-80039e8d5475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load in module pyspark.sql.readwriter:\n\nload(path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads data from a data source and returns it as a :class:`DataFrame`.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str or list, optional\n        optional string or a list of string for file-system backed data sources.\n    format : str, optional\n        optional string for format of the data source. Default to 'parquet'.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        optional :class:`pyspark.sql.types.StructType` for the input schema\n        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    **options : dict\n        all other string options\n    \n    Examples\n    --------\n    Load a CSV file with format, schema and options specified.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a CSV file with a header\n    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n    ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n    ...\n    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n    ...     # and 'header' option set to `True`.\n    ...     df = spark.read.load(\n    ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n    ...     df.printSchema()\n    ...     df.show()\n    root\n     |-- age: long (nullable = true)\n     |-- name: string (nullable = true)\n    +---+----+\n    |age|name|\n    +---+----+\n    |100|NULL|\n    +---+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.format('csv').load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3246dfe-6489-41fc-b9df-1a82150ec5ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method option in module pyspark.sql.readwriter:\n\noption(key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader' method of pyspark.sql.readwriter.DataFrameReader instance\n    Adds an input option for the underlying data source.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    key : str\n        The key for the option to set.\n    value\n        The value for the option to set.\n    \n    Examples\n    --------\n    >>> spark.read.option(\"key\", \"value\")\n    <...readwriter.DataFrameReader object ...>\n    \n    Specify the option 'nullValue' with reading a CSV file.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a CSV file\n    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n    ...\n    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n    ...     spark.read.schema(df.schema).option(\n    ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n    +---+----+\n    |age|name|\n    +---+----+\n    |100|NULL|\n    +---+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ab9752-d3c7-4895-bba0-4146fbd8ea54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark. \\\n",
    "        read.\\\n",
    "        option('sep','|').\\\n",
    "        option('header', None).\\\n",
    "        option('inferSchema', True). \\\n",
    "        csv(f'/user/{username}/retail_db_pipe/orders').\\\n",
    "        toDF('order_id','order_date','order_customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38e9685-faca-4879-95db-0a1c63004ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb19596-b52e-4627-a751-b3bf93d53280",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.read.\\\n",
    "         options(sep = '|', header = None, inferSchema=True).\\\n",
    "         csv(f'/user/{username}/retail_db_pipe/orders').\\\n",
    "         toDF('order_id','order_date','order_customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad988e0e-9f6a-452f-a863-1984d715d4d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd568cb-a9af-44ca-8849-b0f21b79150a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    'sep': '|',\n",
    "    'header': None,\n",
    "    'inferSchema': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a070a4-574b-4578-8770-d6d1348476ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.\\\n",
    "        read.\\\n",
    "        options(**options).\\\n",
    "        csv(f'/user/{username}/retail_db_pipe/orders').\\\n",
    "        toDF('order_id','order_date','order_customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad0fcdd-e388-4b33-9ec4-e35169dd7221",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n|order_id|         order_date|order_customer_id|   order_status|\n+--------+-------------------+-----------------+---------------+\n|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n+--------+-------------------+-----------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "653fc414-fa4f-43f4-99b6-8102d4000dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10 - Using Options while reading CSV Files into Spark Data Frame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

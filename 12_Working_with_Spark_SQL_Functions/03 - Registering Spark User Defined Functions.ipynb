{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d809880a-0b4f-474a-84f7-86dcc47611a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here are the steps we need to follow to develop and use Spark UDFs\n",
    "- Develop required logic using Python as programming language\n",
    "- Register the function using `spark.udf.register` . Also assign it to a variable\n",
    "- Variable can be used as part of Data Frame APIs such as `select`, `filter` etc\n",
    "- When we register, we register with a name. That name can be used as part of `selectExpr` or as part of Spark SQL queries using spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db49b518-260a-4494-a109-75eade7e4ebf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method register in module pyspark.sql.udf:\n\nregister(name: str, f: Union[Callable[..., Any], ForwardRef('UserDefinedFunctionLike')], returnType: Optional[ForwardRef('DataTypeOrString')] = None) -> 'UserDefinedFunctionLike' method of pyspark.sql.udf.UDFRegistration instance\n    Register a Python function (including lambda function) or a user-defined function\n    as a SQL function.\n    \n    .. versionadded:: 1.3.1\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    name : str,\n        name of the user-defined function in SQL statements.\n    f : function, :meth:`pyspark.sql.functions.udf` or :meth:`pyspark.sql.functions.pandas_udf`\n        a Python function, or a user-defined function. The user-defined function can\n        be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n        :meth:`pyspark.sql.functions.pandas_udf`.\n    returnType : :class:`pyspark.sql.types.DataType` or str, optional\n        the return type of the registered user-defined function. The value can\n        be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        `returnType` can be optionally specified when `f` is a Python function but not\n        when `f` is a user-defined function. Please see the examples below.\n    \n    Returns\n    -------\n    function\n        a user-defined function\n    \n    Notes\n    -----\n    To register a nondeterministic Python function, users need to first build\n    a nondeterministic user-defined function for the Python function and then register it\n    as a SQL function.\n    \n    Examples\n    --------\n    1. When `f` is a Python function:\n    \n        `returnType` defaults to string type and can be optionally specified. The produced\n        object must match the specified type. In this case, this API works as if\n        `register(name, f, returnType=StringType())`.\n    \n        >>> strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n        >>> spark.sql(\"SELECT stringLengthString('test')\").collect()\n        [Row(stringLengthString(test)='4')]\n    \n        >>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()\n        [Row(stringLengthString(text)='3')]\n    \n        >>> from pyspark.sql.types import IntegerType\n        >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n        >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n        [Row(stringLengthInt(test)=4)]\n    \n        >>> from pyspark.sql.types import IntegerType\n        >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n        >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n        [Row(stringLengthInt(test)=4)]\n    \n    2. When `f` is a user-defined function (from Spark 2.3.0):\n    \n        Spark uses the return type of the given user-defined function as the return type of\n        the registered user-defined function. `returnType` should not be specified.\n        In this case, this API works as if `register(name, f)`.\n    \n        >>> from pyspark.sql.types import IntegerType\n        >>> from pyspark.sql.functions import udf\n        >>> slen = udf(lambda s: len(s), IntegerType())\n        >>> _ = spark.udf.register(\"slen\", slen)\n        >>> spark.sql(\"SELECT slen('test')\").collect()\n        [Row(slen(test)=4)]\n    \n        >>> import random\n        >>> from pyspark.sql.functions import udf\n        >>> from pyspark.sql.types import IntegerType\n        >>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n        >>> new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n        >>> spark.sql(\"SELECT random_udf()\").collect()  # doctest: +SKIP\n        [Row(random_udf()=82)]\n    \n        >>> import pandas as pd  # doctest: +SKIP\n        >>> from pyspark.sql.functions import pandas_udf\n        >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n        ... def add_one(s: pd.Series) -> pd.Series:\n        ...     return s + 1\n        ...\n        >>> _ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n        >>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  # doctest: +SKIP\n        [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n    \n        >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n        ... def sum_udf(v: pd.Series) -> int:\n        ...     return v.sum()\n        ...\n        >>> _ = spark.udf.register(\"sum_udf\", sum_udf)  # doctest: +SKIP\n        >>> q = \"SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2\"\n        >>> spark.sql(q).collect()  # doctest: +SKIP\n        [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.udf.register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee7902c-ca6e-48a2-91d7-afbc5d33226c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f4c278-b57a-44af-bf24-de737d4c9bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851dbe87-fa41-4be8-88be-18de3c03f3c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dc = spark.udf.register('date_convert', lambda d: int(d[:10].replace('-','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ca98ab-d9a7-464d-b5ba-33675359e336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(d)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978cb050-7cb7-4181-8918-acbcdf86dcd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|order_date|\n+----------+\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(dc('order_date').alias('order_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6665161c-895c-467e-ae47-646077b0aa3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n|             9084|2014-01-01 00:00:...|   25878|        PENDING|\n|             5118|2014-01-01 00:00:...|   25879|        PENDING|\n|            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n|             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n|             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n|            11764|2014-01-01 00:00:...|   25883|        PENDING|\n|             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n|             7253|2014-01-01 00:00:...|   25885|        PENDING|\n|             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n|            10062|2014-01-01 00:00:...|   25887|        PENDING|\n|             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n|            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n|             2581|2014-01-01 00:00:...|   25890|        PENDING|\n|             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n|             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n|             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n|             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n|             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(dc('order_date') == 20140101).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa24f2e-e40e-4f3c-b7f7-905f9ba0cbd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|order_date|order_count|\n+----------+-----------+\n|  20130919|        206|\n|  20140303|        266|\n|  20140202|        192|\n|  20140310|        235|\n|  20130809|        125|\n|  20130817|        253|\n|  20131015|        174|\n|  20140114|        209|\n|  20131029|        128|\n|  20140130|        254|\n|  20130824|        265|\n|  20130913|        103|\n|  20130914|        276|\n|  20130825|        200|\n|  20131031|        208|\n|  20140304|        257|\n|  20130731|        252|\n|  20130730|        227|\n|  20131116|        120|\n|  20131213|        135|\n+----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df. \\\n",
    "    groupBy(dc('order_date').alias('order_date')).\\\n",
    "    count().\\\n",
    "    withColumnRenamed('count','order_count').\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b635d67-5554-4b16-b96f-b0abd735f48e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Registering Spark User Defined Functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2961c9-aaf9-4d11-852e-a9aad61616f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n|course_id|        course_title|course_published_at|is_active|    last_updated_ts|\n+---------+--------------------+-------------------+---------+-------------------+\n|        1|    Mastering python|         2021-01-04|     true|2021-02-18 16:57:25|\n|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n|        3|   Mastering pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n+---------+--------------------+-------------------+---------+-------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+\n|user_id|user_first_name|user_last_name|          user_email|\n+-------+---------------+--------------+--------------------+\n|      1|         Sandra|        Karpov|     skarpov@ovh.net|\n|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n|      5|         Loreen|         Malin|lmalin4@independe...|\n|      6|           Augy|      Christon|      achriston5@mlb|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|\n|      9|        Vassily|         Tamas|vtamas@businesswe...|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n+-------+---------------+--------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+----------+\n|course_enrolmane_id|user_id|course_id|price_paid|\n+-------------------+-------+---------+----------+\n|                  1|     10|        2|      9.99|\n|                  2|      5|        2|      9.99|\n|                  3|      7|        5|      9.99|\n|                  4|      9|        2|      9.99|\n|                  5|      8|        2|      9.99|\n|                  6|      5|        5|     10.99|\n|                  7|      4|        5|     10.99|\n|                  8|      7|        3|     10.99|\n|                  9|      8|        5|     10.99|\n|                 10|      3|        3|     10.99|\n|                 11|      7|        5|     10.99|\n|                 12|      3|        2|      9.99|\n|                 13|      5|        2|      9.99|\n|                 14|      4|        3|     10.99|\n|                 15|      8|        2|      9.99|\n+-------------------+-------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/surajthallapalli@outlook.com/08 Joining Spark Data Frames/02 - Setup Datasets to perform joins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097ad798-444e-4b2d-a78f-5f268ae9516e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n\njoin(other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Joins with another :class:`DataFrame`, using the given join expression.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    other : :class:`DataFrame`\n        Right side of the join\n    on : str, list or :class:`Column`, optional\n        a string for the join column name, a list of column names,\n        a join expression (Column), or a list of Columns.\n        If `on` is a string or a list of strings indicating the name of the join column(s),\n        the column(s) must exist on both sides, and this performs an equi-join.\n    how : str, optional\n        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n        ``anti``, ``leftanti`` and ``left_anti``.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        Joined DataFrame.\n    \n    Examples\n    --------\n    The following performs a full outer join between ``df1`` and ``df2``.\n    \n    >>> from pyspark.sql import Row\n    >>> from pyspark.sql.functions import desc\n    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n    >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n    >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n    >>> df4 = spark.createDataFrame([\n    ...     Row(age=10, height=80, name=\"Alice\"),\n    ...     Row(age=5, height=None, name=\"Bob\"),\n    ...     Row(age=None, height=None, name=\"Tom\"),\n    ...     Row(age=None, height=None, name=None),\n    ... ])\n    \n    Inner join on columns (default)\n    \n    >>> df.join(df2, 'name').select(df.name, df2.height).show()\n    +----+------+\n    |name|height|\n    +----+------+\n    | Bob|    85|\n    +----+------+\n    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n    +----+---+\n    |name|age|\n    +----+---+\n    | Bob|  5|\n    +----+---+\n    \n    Outer join for both DataFrames on the 'name' column.\n    \n    >>> df.join(df2, df.name == df2.name, 'outer').select(\n    ...     df.name, df2.height).sort(desc(\"name\")).show()\n    +-----+------+\n    | name|height|\n    +-----+------+\n    |  Bob|    85|\n    |Alice|  NULL|\n    | NULL|    80|\n    +-----+------+\n    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n    +-----+------+\n    | name|height|\n    +-----+------+\n    |  Tom|    80|\n    |  Bob|    85|\n    |Alice|  NULL|\n    +-----+------+\n    \n    Outer join for both DataFrams with multiple columns.\n    \n    >>> df.join(\n    ...     df3,\n    ...     [df.name == df3.name, df.age == df3.age],\n    ...     'outer'\n    ... ).select(df.name, df3.age).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  2|\n    |  Bob|  5|\n    +-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e826e5-33f7-403a-9672-66b47e736ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Get the user details who have enrolled for the ourses\n",
    "  - Need to join **users_df** and **course_enrolments_df**\n",
    "  - Here are the fields that needs to be displayed\n",
    "    - All fields from users_df\n",
    "    - course_id and course_enrolment_id from course_enrolments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7450d1-9420-49f8-aa32-b7353601f9e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolmane_id|user_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|      3|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|      3|        2|      9.99|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                  7|      4|        5|     10.99|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                 14|      4|        3|     10.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                  2|      5|        2|      9.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                  6|      5|        5|     10.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                 13|      5|        2|      9.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  3|      7|        5|      9.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  8|      7|        3|     10.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                 11|      7|        5|     10.99|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|                  5|      8|        2|      9.99|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|                  9|      8|        5|     10.99|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|                 15|      8|        2|      9.99|\n|      9|        Vassily|         Tamas|vtamas@businesswe...|                  4|      9|        2|      9.99|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|                  1|     10|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+-------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "users_df.\\\n",
    "    join(course_enrolments_df, users_df.user_id == course_enrolments_df.user_id).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48be161e-4070-4989-b65e-c63542c08479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n|user_id|user_first_name|user_last_name|          user_email|course_enrolmane_id|course_id|price_paid|\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 10|        3|     10.99|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|                 12|        2|      9.99|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                  7|        5|     10.99|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|                 14|        3|     10.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                  2|        2|      9.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                  6|        5|     10.99|\n|      5|         Loreen|         Malin|lmalin4@independe...|                 13|        2|      9.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  3|        5|      9.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                  8|        3|     10.99|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|                 11|        5|     10.99|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|                  5|        2|      9.99|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|                  9|        5|     10.99|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|                 15|        2|      9.99|\n|      9|        Vassily|         Tamas|vtamas@businesswe...|                  4|        2|      9.99|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|                  1|        2|      9.99|\n+-------+---------------+--------------+--------------------+-------------------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#as both data frames have user id using same name, we can pass column name as string as well. Advantage of this approach is the condition column is mentioned once in df unlike above approach\n",
    "users_df.\\\n",
    "    join(course_enrolments_df, 'user_id').\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29cae8c6-c53b-4f04-900c-54c71e30f17b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- To get all columns from users_df, course_id and course_enrolmane_id from course_enrolments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc7d2ae-6998-427f-88ca-c91ecd73dcb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+---------+-------------------+\n|user_id|user_first_name|user_last_name|          user_email|course_id|course_enrolmane_id|\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        3|                 10|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        2|                 12|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|        5|                  7|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|        3|                 14|\n|      5|         Loreen|         Malin|lmalin4@independe...|        2|                  2|\n|      5|         Loreen|         Malin|lmalin4@independe...|        5|                  6|\n|      5|         Loreen|         Malin|lmalin4@independe...|        2|                 13|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        5|                  3|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        3|                  8|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        5|                 11|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|        2|                  5|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|        5|                  9|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|        2|                 15|\n|      9|        Vassily|         Tamas|vtamas@businesswe...|        2|                  4|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|        2|                  1|\n+-------+---------------+--------------+--------------------+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "users_df.\\\n",
    "    join(course_enrolments_df, users_df.user_id == course_enrolments_df.user_id).\\\n",
    "    select(users_df['*'],course_enrolments_df['course_id'],course_enrolments_df['course_enrolmane_id']).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a8d52d0-1898-4111-be46-bc70e591c575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+---------+-------------------+\n|user_id|user_first_name|user_last_name|          user_email|course_id|course_enrolmane_id|\n+-------+---------------+--------------+--------------------+---------+-------------------+\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        3|                 10|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|        2|                 12|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|        5|                  7|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|        3|                 14|\n|      5|         Loreen|         Malin|lmalin4@independe...|        2|                  2|\n|      5|         Loreen|         Malin|lmalin4@independe...|        5|                  6|\n|      5|         Loreen|         Malin|lmalin4@independe...|        2|                 13|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        5|                  3|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        3|                  8|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|        5|                 11|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|        2|                  5|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|        5|                  9|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|        2|                 15|\n|      9|        Vassily|         Tamas|vtamas@businesswe...|        2|                  4|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|        2|                  1|\n+-------+---------------+--------------+--------------------+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# using alias\n",
    "users_df.alias('u').\\\n",
    "    join(course_enrolments_df.alias('ce'), users_df.user_id == course_enrolments_df.user_id).\\\n",
    "    select('u.*','course_id','course_enrolmane_id').\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa987c3b-9c8f-43cb-abdb-985872b796f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- Get number of courses enrolled by each user. Fails as user_id is part of both the data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6003228c-0ce5-4434-abd4-cc3b70093463",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-638244179228379>, line 5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# using alias\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43musers_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcourse_enrolments_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mce\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musers_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_id\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcourse_enrolments_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_id\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroupBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muser_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n",
       "\u001B[0;32m----> 5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m\\\n",
       "\u001B[1;32m      6\u001B[0m     show()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/group.py:38\u001B[0m, in \u001B[0;36mdfapi.<locals>._api\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_api\u001B[39m(\u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGroupedData\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n",
       "\u001B[1;32m     37\u001B[0m     name \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n",
       "\u001B[0;32m---> 38\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jgd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [AMBIGUOUS_REFERENCE] Reference `user_id` is ambiguous, could be: [`ce`.`user_id`, `u`.`user_id`]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-638244179228379>, line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# using alias\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[43musers_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcourse_enrolments_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mce\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musers_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_id\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcourse_enrolments_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_id\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroupBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muser_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n\u001B[0;32m----> 5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m\\\n\u001B[1;32m      6\u001B[0m     show()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/group.py:38\u001B[0m, in \u001B[0;36mdfapi.<locals>._api\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_api\u001B[39m(\u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGroupedData\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m     37\u001B[0m     name \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m---> 38\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jgd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [AMBIGUOUS_REFERENCE] Reference `user_id` is ambiguous, could be: [`ce`.`user_id`, `u`.`user_id`].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [AMBIGUOUS_REFERENCE] Reference `user_id` is ambiguous, could be: [`ce`.`user_id`, `u`.`user_id`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using alias\n",
    "users_df.alias('u').\\\n",
    "    join(course_enrolments_df.alias('ce'), users_df.user_id == course_enrolments_df.user_id).\\\n",
    "    groupBy('user_id').\\\n",
    "    count().\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e95b49b6-e640-4afe-8aff-1fdf5b5b5520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id|count|\n+-------+-----+\n|      3|    2|\n|      4|    2|\n|      5|    3|\n|      7|    3|\n|      8|    3|\n|      9|    1|\n|     10|    1|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "users_df.alias('u').\\\n",
    "    join(course_enrolments_df.alias('ce'), users_df.user_id == course_enrolments_df.user_id).\\\n",
    "    groupBy(users_df['user_id']).\\\n",
    "    count().\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a2f4de-2883-4752-be9f-1305acff615a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id|count|\n+-------+-----+\n|      3|    2|\n|      4|    2|\n|      5|    3|\n|      7|    3|\n|      8|    3|\n|      9|    1|\n|     10|    1|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# using alias\n",
    "users_df.alias('u').\\\n",
    "    join(course_enrolments_df.alias('ce'), users_df.user_id == course_enrolments_df.user_id).\\\n",
    "    groupBy('u.user_id').\\\n",
    "    count().\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664858f9-3762-4ee7-bb7c-cea8e20f1572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id|count|\n+-------+-----+\n|      3|    2|\n|      4|    2|\n|      5|    3|\n|      7|    3|\n|      8|    3|\n|      9|    1|\n|     10|    1|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "users_df.\\\n",
    "    join(course_enrolments_df, 'user_id').\\\n",
    "    groupBy('user_id').\\\n",
    "    count().\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1394706-1294-4e48-adad-3dded8c9ded6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05 - Performing Inner Join on Spark DataFrames",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

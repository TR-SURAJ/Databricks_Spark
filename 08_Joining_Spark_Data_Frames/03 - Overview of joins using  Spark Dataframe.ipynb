{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "043915f9-7114-41e0-8902-f85ec284d579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n|course_id|        course_title|course_published_at|is_active|    last_updated_ts|\n+---------+--------------------+-------------------+---------+-------------------+\n|        1|    Mastering python|         2021-01-04|     true|2021-02-18 16:57:25|\n|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n|        3|   Mastering pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n+---------+--------------------+-------------------+---------+-------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+\n|user_id|user_first_name|user_last_name|          user_email|\n+-------+---------------+--------------+--------------------+\n|      1|         Sandra|        Karpov|     skarpov@ovh.net|\n|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n|      5|         Loreen|         Malin|lmalin4@independe...|\n|      6|           Augy|      Christon|      achriston5@mlb|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|\n|      9|        Vassily|         Tamas|vtamas@businesswe...|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n+-------+---------------+--------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+----------+\n|course_enrolmane_id|user_id|course_id|price_paid|\n+-------------------+-------+---------+----------+\n|                  1|     10|        2|      9.99|\n|                  2|      5|        2|      9.99|\n|                  3|      7|        5|      9.99|\n|                  4|      9|        2|      9.99|\n|                  5|      8|        2|      9.99|\n|                  6|      5|        5|     10.99|\n|                  7|      4|        5|     10.99|\n|                  8|      7|        3|     10.99|\n|                  9|      8|        5|     10.99|\n|                 10|      3|        3|     10.99|\n|                 11|      7|        5|     10.99|\n|                 12|      3|        2|      9.99|\n|                 13|      5|        2|      9.99|\n|                 14|      4|        3|     10.99|\n|                 15|      8|        2|      9.99|\n+-------------------+-------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/surajthallapalli@outlook.com/08 Joining Spark Data Frames/02 - Setup Datasets to perform joins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc197f1-92dd-4d81-b2ed-6543aa6081cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n\njoin(other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Joins with another :class:`DataFrame`, using the given join expression.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    other : :class:`DataFrame`\n        Right side of the join\n    on : str, list or :class:`Column`, optional\n        a string for the join column name, a list of column names,\n        a join expression (Column), or a list of Columns.\n        If `on` is a string or a list of strings indicating the name of the join column(s),\n        the column(s) must exist on both sides, and this performs an equi-join.\n    how : str, optional\n        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n        ``anti``, ``leftanti`` and ``left_anti``.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        Joined DataFrame.\n    \n    Examples\n    --------\n    The following performs a full outer join between ``df1`` and ``df2``.\n    \n    >>> from pyspark.sql import Row\n    >>> from pyspark.sql.functions import desc\n    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n    >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n    >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n    >>> df4 = spark.createDataFrame([\n    ...     Row(age=10, height=80, name=\"Alice\"),\n    ...     Row(age=5, height=None, name=\"Bob\"),\n    ...     Row(age=None, height=None, name=\"Tom\"),\n    ...     Row(age=None, height=None, name=None),\n    ... ])\n    \n    Inner join on columns (default)\n    \n    >>> df.join(df2, 'name').select(df.name, df2.height).show()\n    +----+------+\n    |name|height|\n    +----+------+\n    | Bob|    85|\n    +----+------+\n    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n    +----+---+\n    |name|age|\n    +----+---+\n    | Bob|  5|\n    +----+---+\n    \n    Outer join for both DataFrames on the 'name' column.\n    \n    >>> df.join(df2, df.name == df2.name, 'outer').select(\n    ...     df.name, df2.height).sort(desc(\"name\")).show()\n    +-----+------+\n    | name|height|\n    +-----+------+\n    |  Bob|    85|\n    |Alice|  NULL|\n    | NULL|    80|\n    +-----+------+\n    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n    +-----+------+\n    | name|height|\n    +-----+------+\n    |  Tom|    80|\n    |  Bob|    85|\n    |Alice|  NULL|\n    +-----+------+\n    \n    Outer join for both DataFrams with multiple columns.\n    \n    >>> df.join(\n    ...     df3,\n    ...     [df.name == df3.name, df.age == df3.age],\n    ...     'outer'\n    ... ).select(df.name, df3.age).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  2|\n    |  Bob|  5|\n    +-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272a897d-96e5-4b3d-8c41-d61aa78a5182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method crossJoin in module pyspark.sql.dataframe:\n\ncrossJoin(other: 'DataFrame') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns the cartesian product with another :class:`DataFrame`.\n    \n    .. versionadded:: 2.1.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    other : :class:`DataFrame`\n        Right side of the cartesian product.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        Joined DataFrame.\n    \n    Examples\n    --------\n    >>> from pyspark.sql import Row\n    >>> df = spark.createDataFrame(\n    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n    >>> df2 = spark.createDataFrame(\n    ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n    >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n    +---+-----+------+\n    |age| name|height|\n    +---+-----+------+\n    | 14|  Tom|    80|\n    | 14|  Tom|    85|\n    | 23|Alice|    80|\n    | 23|Alice|    85|\n    | 16|  Bob|    80|\n    | 16|  Bob|    85|\n    +---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.crossJoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c451c1a8-ff9b-479a-a5fa-dad223860f90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Overview of joins using  Spark Dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

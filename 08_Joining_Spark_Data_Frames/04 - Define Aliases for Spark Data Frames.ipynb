{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a3a162-5b74-406b-ba73-11c4f46af9a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n|course_id|        course_title|course_published_at|is_active|    last_updated_ts|\n+---------+--------------------+-------------------+---------+-------------------+\n|        1|    Mastering python|         2021-01-04|     true|2021-02-18 16:57:25|\n|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n|        3|   Mastering pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n+---------+--------------------+-------------------+---------+-------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+--------------------+\n|user_id|user_first_name|user_last_name|          user_email|\n+-------+---------------+--------------+--------------------+\n|      1|         Sandra|        Karpov|     skarpov@ovh.net|\n|      2|           Kari|        Dearth|kdearth1@so-net.n...|\n|      3|         Joanna|      Spennock|jspennock2@redcro...|\n|      4|         Hirsch|       Conaboy|hconaboy3@barnesa...|\n|      5|         Loreen|         Malin|lmalin4@independe...|\n|      6|           Augy|      Christon|      achriston5@mlb|\n|      7|         Trudey|       Choupin|     tchoupin6@de.vu|\n|      8|         Nadine|     Grimsdell| ngrimsdell7@ohu.com|\n|      9|        Vassily|         Tamas|vtamas@businesswe...|\n|     10|          Wells|      Simpkins|wsimpkins9@amazon...|\n+-------+---------------+--------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+----------+\n|course_enrolmane_id|user_id|course_id|price_paid|\n+-------------------+-------+---------+----------+\n|                  1|     10|        2|      9.99|\n|                  2|      5|        2|      9.99|\n|                  3|      7|        5|      9.99|\n|                  4|      9|        2|      9.99|\n|                  5|      8|        2|      9.99|\n|                  6|      5|        5|     10.99|\n|                  7|      4|        5|     10.99|\n|                  8|      7|        3|     10.99|\n|                  9|      8|        5|     10.99|\n|                 10|      3|        3|     10.99|\n|                 11|      7|        5|     10.99|\n|                 12|      3|        2|      9.99|\n|                 13|      5|        2|      9.99|\n|                 14|      4|        3|     10.99|\n|                 15|      8|        2|      9.99|\n+-------------------+-------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/surajthallapalli@outlook.com/08 Joining Spark Data Frames/02 - Setup Datasets to perform joins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95842b88-4fe2-43ba-8f26-ac6aeb323424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method alias in module pyspark.sql.dataframe:\n\nalias(alias: str) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` with an alias set.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    alias : str\n        an alias name to be set for the :class:`DataFrame`.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        Aliased DataFrame.\n    \n    Examples\n    --------\n    >>> from pyspark.sql.functions import col, desc\n    >>> df = spark.createDataFrame(\n    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n    >>> df_as1 = df.alias(\"df_as1\")\n    >>> df_as2 = df.alias(\"df_as2\")\n    >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n    >>> joined_df.select(\n    ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n    +-----+-----+---+\n    | name| name|age|\n    +-----+-----+---+\n    |  Tom|  Tom| 14|\n    |  Bob|  Bob| 16|\n    |Alice|Alice| 23|\n    +-----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfbb615-740b-4e96-853c-4f5eca8be532",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(courses_df.alias('c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c280276-7c04-41f6-9db5-bb9777a41b37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|course_id|\n+---------+\n|        1|\n|        2|\n|        3|\n|        4|\n|        5|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "courses_df.alias('c').select('c.course_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68a030e9-8f68-4d8c-a048-2b0d7a9c2b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04 - Define Aliases for Spark Data Frames",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

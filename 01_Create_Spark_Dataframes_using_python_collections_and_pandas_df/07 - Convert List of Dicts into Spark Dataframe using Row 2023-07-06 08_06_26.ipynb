{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76ca9b8-2e36-4074-9645-aba7128f7ccc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users_list = [\n",
    "    {'user_id': 1, 'user_first_name': 'Scott'},\n",
    "    {'user_id': 2, 'user_first_name': 'bob'},\n",
    "    {'user_id': 3, 'user_first_name': 'rick'},\n",
    "    {'user_id': 4, 'user_first_name': 'bonnie'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85c6eae-a2bd-4bae-bac6-acd1d5898605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: DataFrame[user_first_name: string, user_id: bigint]"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361e060e-6313-41b5-b0b6-3dbfe886de7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14a76b7-2cb0-4613-a510-96bf8763ebcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n\nclass Row(builtins.tuple)\n |  Row(*args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n |  \n |  A row in :class:`DataFrame`.\n |  The fields in it can be accessed:\n |  \n |  * like attributes (``row.key``)\n |  * like dictionary values (``row[key]``)\n |  \n |  ``key in row`` will search through row keys.\n |  \n |  Row can be used to create a row object by using named arguments.\n |  It is not allowed to omit a named argument to represent that the value is\n |  None or missing. This should be explicitly set to None in this case.\n |  \n |  .. versionchanged:: 3.0.0\n |      Rows created from named arguments no longer have\n |      field names sorted alphabetically and will be ordered in the position as\n |      entered.\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.sql import Row\n |  >>> row = Row(name=\"Alice\", age=11)\n |  >>> row\n |  Row(name='Alice', age=11)\n |  >>> row['name'], row['age']\n |  ('Alice', 11)\n |  >>> row.name, row.age\n |  ('Alice', 11)\n |  >>> 'name' in row\n |  True\n |  >>> 'wrong_key' in row\n |  False\n |  \n |  Row also can be used to create another Row like class, then it\n |  could be used to create Row objects, such as\n |  \n |  >>> Person = Row(\"name\", \"age\")\n |  >>> Person\n |  <Row('name', 'age')>\n |  >>> 'name' in Person\n |  True\n |  >>> 'wrong_key' in Person\n |  False\n |  >>> Person(\"Alice\", 11)\n |  Row(name='Alice', age=11)\n |  \n |  This form can also be used to create rows as tuple values, i.e. with unnamed\n |  fields.\n |  \n |  >>> row1 = Row(\"Alice\", 11)\n |  >>> row2 = Row(name=\"Alice\", age=11)\n |  >>> row1 == row2\n |  True\n |  \n |  Method resolution order:\n |      Row\n |      builtins.tuple\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __call__(self, *args: Any) -> 'Row'\n |      create new Row object\n |  \n |  __contains__(self, item: Any) -> bool\n |      Return key in self.\n |  \n |  __getattr__(self, item: str) -> Any\n |  \n |  __getitem__(self, item: Any) -> Any\n |      Return self[key].\n |  \n |  __reduce__(self) -> Union[str, Tuple[Any, ...]]\n |      Returns a tuple so Python knows how to pickle Row.\n |  \n |  __repr__(self) -> str\n |      Printable representation of Row used in Python REPL.\n |  \n |  __setattr__(self, key: Any, value: Any) -> None\n |      Implement setattr(self, name, value).\n |  \n |  asDict(self, recursive: bool = False) -> Dict[str, Any]\n |      Return as a dict\n |      \n |      Parameters\n |      ----------\n |      recursive : bool, optional\n |          turns the nested Rows to dict (default: False).\n |      \n |      Notes\n |      -----\n |      If a row contains duplicate field names, e.g., the rows of a join\n |      between two :class:`DataFrame` that both have the fields of same names,\n |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n |      will also return one of the duplicate fields, however returned value might\n |      be different to ``asDict``.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n |      True\n |      >>> row = Row(key=1, value=Row(name='a', age=2))\n |      >>> row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n |      True\n |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n |      True\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from builtins.tuple:\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __ge__(self, value, /)\n |      Return self>=value.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __getnewargs__(self, /)\n |  \n |  __gt__(self, value, /)\n |      Return self>value.\n |  \n |  __hash__(self, /)\n |      Return hash(self).\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __le__(self, value, /)\n |      Return self<=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lt__(self, value, /)\n |      Return self<value.\n |  \n |  __mul__(self, value, /)\n |      Return self*value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __rmul__(self, value, /)\n |      Return value*self.\n |  \n |  count(self, value, /)\n |      Return number of occurrences of value.\n |  \n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from builtins.tuple:\n |  \n |  __class_getitem__(...) from builtins.type\n |      See PEP 585\n\n"
     ]
    }
   ],
   "source": [
    "help(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a237ed4-d305-4890-a3c5-a8c101f37ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users_rows = [Row(*user.values()) for user in users_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5066759a-f540-479c-97e7-668221ef4cfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [<Row(1, 'Scott')>, <Row(2, 'bob')>, <Row(3, 'rick')>, <Row(4, 'bonnie')>]"
     ]
    }
   ],
   "source": [
    "users_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b81664-33cf-47db-bd87-317a29db02e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(users_rows,'id int, name string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3d2f59-4138-4a50-b4f7-6a62f90dbdf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1| Scott|\n|  2|   bob|\n|  3|  rick|\n|  4|bonnie|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb40270-b811-4004-ad75-615c9d295003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_rows_2 = [Row(**user)  for user in users_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6629ef93-66ce-4afb-8c0e-7c18fdd7107a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: [Row(user_id=1, user_first_name='Scott'),\n Row(user_id=2, user_first_name='bob'),\n Row(user_id=3, user_first_name='rick'),\n Row(user_id=4, user_first_name='bonnie')]"
     ]
    }
   ],
   "source": [
    "user_rows_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463b9bc0-1ac2-4162-96ec-c095d43b7031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(user_rows_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89dda1ac-7691-4403-b8c8-fb7e609b33f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n|user_id|user_first_name|\n+-------+---------------+\n|      1|          Scott|\n|      2|            bob|\n|      3|           rick|\n|      4|         bonnie|\n+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2f36c50-4ba0-471f-985c-03d5273d63ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07 - Convert List of Dicts into Spark Dataframe using Row 2023-07-06 08:06:26",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

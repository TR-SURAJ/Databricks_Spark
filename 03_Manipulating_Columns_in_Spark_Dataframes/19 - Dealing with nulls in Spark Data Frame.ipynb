{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7be4b08-6606-4184-b267-bcb2bef1baa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "887fa921-60ec-4b3f-b2e1-71edd00495f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, None, \"Tiger\", None,30,\"united states\", \"+1 123 456 7890\", \"123 45 6789\"),\n",
    "    (2, \"Henry\", \"Ford\", 1250.0,None, \"india\", \"+91 234 567 8901\", \"456 78 9123\"),\n",
    "    (3, \"Nick\", \"Junior\",500.0,\"\", \"united kingdom\", \"+44 111 111 111\", \"222 33 4444\"),\n",
    "    (4, \"Bill\", \"Gomes\", 1500.0,150, \"australia\", \"+61 987 654 3210\", \"789 12 6113\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db1ce27-41d7-464a-9869-e48ae3722c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empdf = spark.createDataFrame(employees, schema = \"\"\"employee_id INT, first_name STRING,\n",
    "                              last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                              phone_number STRING, ssn STRING                              \n",
    "                               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1196ba-c7f9-413a-b409-62549fc44f32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8ff6f1-26ef-4c37-a0dc-98c294c2f99d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function coalesce in module pyspark.sql.functions:\n\ncoalesce(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n    Returns the first column that is not null.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    cols : :class:`~pyspark.sql.Column` or str\n        list of columns to work on.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        value of the first column that is not null.\n    \n    Examples\n    --------\n    >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n    >>> cDf.show()\n    +----+----+\n    |   a|   b|\n    +----+----+\n    |NULL|NULL|\n    |   1|NULL|\n    |NULL|   2|\n    +----+----+\n    \n    >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n    +--------------+\n    |coalesce(a, b)|\n    +--------------+\n    |          NULL|\n    |             1|\n    |             2|\n    +--------------+\n    \n    >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n    +----+----+----------------+\n    |   a|   b|coalesce(a, 0.0)|\n    +----+----+----------------+\n    |NULL|NULL|             0.0|\n    |   1|NULL|             1.0|\n    |NULL|   2|             0.0|\n    +----+----+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "help(coalesce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d166f162-e9c6-435e-9c7e-71b21460ada1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1590524349081141>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Fails because 0 is not passed as column object. But using lit it will look for a column with name 0\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m empdf\u001B[38;5;241m.\u001B[39m \\\n",
       "\u001B[0;32m----> 3\u001B[0m     withColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbonus1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43mcoalesce\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbonus\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39m \\\n",
       "\u001B[1;32m      4\u001B[0m     show()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:162\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:3803\u001B[0m, in \u001B[0;36mcoalesce\u001B[0;34m(*cols)\u001B[0m\n",
       "\u001B[1;32m   3754\u001B[0m \u001B[38;5;129m@try_remote_functions\u001B[39m\n",
       "\u001B[1;32m   3755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcoalesce\u001B[39m(\u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Column:\n",
       "\u001B[1;32m   3756\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the first column that is not null.\u001B[39;00m\n",
       "\u001B[1;32m   3757\u001B[0m \n",
       "\u001B[1;32m   3758\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.4.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3801\u001B[0m \u001B[38;5;124;03m    +----+----+----------------+\u001B[39;00m\n",
       "\u001B[1;32m   3802\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_invoke_function_over_seq_of_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcoalesce\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:115\u001B[0m, in \u001B[0;36m_invoke_function_over_seq_of_columns\u001B[0;34m(name, cols)\u001B[0m\n",
       "\u001B[1;32m    110\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03mInvokes unary JVM function identified by name with\u001B[39;00m\n",
       "\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001B[39;00m\n",
       "\u001B[1;32m    113\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    114\u001B[0m sc \u001B[38;5;241m=\u001B[39m get_active_spark_context()\n",
       "\u001B[0;32m--> 115\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _invoke_function(name, \u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_to_java_column\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:89\u001B[0m, in \u001B[0;36m_to_seq\u001B[0;34m(sc, cols, converter)\u001B[0m\n",
       "\u001B[1;32m     82\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n",
       "\u001B[1;32m     84\u001B[0m \n",
       "\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n",
       "\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n",
       "\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n",
       "\u001B[0;32m---> 89\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n",
       "\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:89\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n",
       "\u001B[1;32m     82\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n",
       "\u001B[1;32m     84\u001B[0m \n",
       "\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n",
       "\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n",
       "\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n",
       "\u001B[0;32m---> 89\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [\u001B[43mconverter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n",
       "\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:66\u001B[0m, in \u001B[0;36m_to_java_column\u001B[0;34m(col)\u001B[0m\n",
       "\u001B[1;32m     64\u001B[0m     jcol \u001B[38;5;241m=\u001B[39m _create_column_from_name(col)\n",
       "\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m     67\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN_OR_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     68\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m     69\u001B[0m     )\n",
       "\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m jcol\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\nFile \u001B[0;32m<command-1590524349081141>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Fails because 0 is not passed as column object. But using lit it will look for a column with name 0\u001B[39;00m\n\u001B[1;32m      2\u001B[0m empdf\u001B[38;5;241m.\u001B[39m \\\n\u001B[0;32m----> 3\u001B[0m     withColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbonus1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43mcoalesce\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbonus\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39m \\\n\u001B[1;32m      4\u001B[0m     show()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:162\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:3803\u001B[0m, in \u001B[0;36mcoalesce\u001B[0;34m(*cols)\u001B[0m\n\u001B[1;32m   3754\u001B[0m \u001B[38;5;129m@try_remote_functions\u001B[39m\n\u001B[1;32m   3755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcoalesce\u001B[39m(\u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Column:\n\u001B[1;32m   3756\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the first column that is not null.\u001B[39;00m\n\u001B[1;32m   3757\u001B[0m \n\u001B[1;32m   3758\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.4.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;124;03m    +----+----+----------------+\u001B[39;00m\n\u001B[1;32m   3802\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_invoke_function_over_seq_of_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcoalesce\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:115\u001B[0m, in \u001B[0;36m_invoke_function_over_seq_of_columns\u001B[0;34m(name, cols)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03mInvokes unary JVM function identified by name with\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    114\u001B[0m sc \u001B[38;5;241m=\u001B[39m get_active_spark_context()\n\u001B[0;32m--> 115\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _invoke_function(name, \u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_to_java_column\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:89\u001B[0m, in \u001B[0;36m_to_seq\u001B[0;34m(sc, cols, converter)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \n\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n\u001B[0;32m---> 89\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:89\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \n\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n\u001B[0;32m---> 89\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [\u001B[43mconverter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:66\u001B[0m, in \u001B[0;36m_to_java_column\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m     64\u001B[0m     jcol \u001B[38;5;241m=\u001B[39m _create_column_from_name(col)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m     67\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN_OR_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     68\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m     69\u001B[0m     )\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m jcol\n\n\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.",
       "errorSummary": "<span class='ansi-red-fg'>PySparkTypeError</span>: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fails because 0 is not passed as column object. But using lit it will look for a column with name 0\n",
    "empdf. \\\n",
    "    withColumn('bonus1', coalesce('bonus', 0)). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a1a09c-fb93-4d85-9244-d971d5447d19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus1|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|    30|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|     0|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|      |\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|   150|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf. \\\n",
    "    withColumn('bonus1', coalesce('bonus', lit(0))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70f8b264-0b86-4045-8588-b2e11f00d1fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus1|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|    30|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|  NULL|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|  NULL|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|   150|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf. \\\n",
    "    withColumn('bonus1', col('bonus').cast('int')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8febfc3-9ba2-4119-b987-41cdd27b0521",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus1|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|    30|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|     0|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|     0|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|   150|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf. \\\n",
    "    withColumn('bonus1', coalesce(col('bonus').cast('int'), lit(0))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5cbe63-9b38-4895-b56d-f97b2b000d93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus1|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|    30|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|     0|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|      |\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|   150|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf. \\\n",
    "    withColumn('bonus1', expr(\"nvl(bonus,0)\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c641ed8c-9a65-4cac-9511-f3a7a28a4039",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus1|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|    30|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|     0|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|     0|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|   150|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf. \\\n",
    "    withColumn('bonus1', expr(\"nvl(nullif(bonus,''),0)\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f68f9bf-1f78-465f-999b-f3e9914d95d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|payment|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|   NULL|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123| 1250.0|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|   NULL|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113| 3750.0|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf. \\\n",
    "    withColumn('payment', col('salary') + ( col('salary') * coalesce(col('bonus'), lit(0)) / 100 )). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4994b0a7-97b5-436a-b77c-3e1c2b1bddd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|payment|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|   NULL|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123| 1250.0|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|  500.0|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113| 3750.0|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf. \\\n",
    "    withColumn('payment', col('salary') + ( col('salary') * coalesce(col('bonus').cast('int'), lit(0)) / 100 )). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50377db4-6d2c-454f-bbf6-22bc84839db1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|payment|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n|          1|      NULL|    Tiger|  NULL|   30| united states| +1 123 456 7890|123 45 6789|    0.0|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123| 1250.0|\n|          3|      Nick|   Junior| 500.0|     |united kingdom| +44 111 111 111|222 33 4444|  500.0|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113| 3750.0|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf. \\\n",
    "    withColumn('payment', coalesce(col('salary'),lit(0)) + ( coalesce(col('salary'),lit(0)) * coalesce(col('bonus').cast('int'), lit(0)) / 100 )). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf5217a-4976-49d7-a1f2-cf4224194b85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", None, 1000.0,30,\"united states\", \"+1 123 456 7890\", \"123 45 6789\"),\n",
    "    (2, \"Henry\", \"Ford\", 1250.0, None, \"india\", \"+91 234 567 8901\", \"456 78 9123\"),\n",
    "    (3, \"Nick\", None, None,\"\", \"united kingdom\", \"+44 111 111 111\", \"222 33 4444\"),\n",
    "    (4, \"Bill\", \"Gomes\", 1500.0,150, \"australia\", \"+61 987 654 3210\", \"789 12 6113\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca3c08a-e92d-4316-9c18-69b295484389",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empdf = spark.createDataFrame(employees, schema = \"\"\"employee_id INT, first_name STRING,\n",
    "                              last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                              phone_number STRING, ssn STRING                              \n",
    "                               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b686b8ce-41da-46d2-8228-54b949f427aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|     NULL|1000.0|   30| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|\n|          3|      Nick|     NULL|  NULL|     |united kingdom| +44 111 111 111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51cee619-848f-4dd6-8396-86f53df2f2f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameNaFunctions in module pyspark.sql.dataframe object:\n\nclass DataFrameNaFunctions(builtins.object)\n |  DataFrameNaFunctions(df: pyspark.sql.dataframe.DataFrame)\n |  \n |  Functionality for working with missing data in :class:`DataFrame`.\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  .. versionchanged:: 3.4.0\n |      Supports Spark Connect.\n |  \n |  Methods defined here:\n |  \n |  __init__(self, df: pyspark.sql.dataframe.DataFrame)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  drop(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n |      Returns a new :class:`DataFrame` omitting rows with null values.\n |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n |      \n |      .. versionadded:: 1.3.1\n |      \n |      .. versionchanged:: 3.4.0\n |          Supports Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      how : str, optional\n |          'any' or 'all'.\n |          If 'any', drop a row if it contains any nulls.\n |          If 'all', drop a row only if all its values are null.\n |      thresh: int, optional\n |          default None\n |          If specified, drop rows that have less than `thresh` non-null values.\n |          This overwrites the `how` parameter.\n |      subset : str, tuple or list, optional\n |          optional list of column names to consider.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          DataFrame with null only rows excluded.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([\n |      ...     Row(age=10, height=80, name=\"Alice\"),\n |      ...     Row(age=5, height=None, name=\"Bob\"),\n |      ...     Row(age=None, height=None, name=\"Tom\"),\n |      ...     Row(age=None, height=None, name=None),\n |      ... ])\n |      >>> df.na.drop().show()\n |      +---+------+-----+\n |      |age|height| name|\n |      +---+------+-----+\n |      | 10|    80|Alice|\n |      +---+------+-----+\n |  \n |  fill(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame\n |      Replace null values, alias for ``na.fill()``.\n |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n |      \n |      .. versionadded:: 1.3.1\n |      \n |      .. versionchanged:: 3.4.0\n |          Supports Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      value : int, float, string, bool or dict\n |          Value to replace null values with.\n |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n |          from column name (string) to replacement value. The replacement value must be\n |          an int, float, boolean, or string.\n |      subset : str, tuple or list, optional\n |          optional list of column names to consider.\n |          Columns specified in subset that do not have matching data types are ignored.\n |          For example, if `value` is a string, and subset contains a non-string column,\n |          then the non-string column is simply ignored.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          DataFrame with replaced null values.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([\n |      ...     (10, 80.5, \"Alice\", None),\n |      ...     (5, None, \"Bob\", None),\n |      ...     (None, None, \"Tom\", None),\n |      ...     (None, None, None, True)],\n |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n |      \n |      Fill all null values with 50 for numeric columns.\n |      \n |      >>> df.na.fill(50).show()\n |      +---+------+-----+----+\n |      |age|height| name|bool|\n |      +---+------+-----+----+\n |      | 10|  80.5|Alice|NULL|\n |      |  5|  50.0|  Bob|NULL|\n |      | 50|  50.0|  Tom|NULL|\n |      | 50|  50.0| NULL|true|\n |      +---+------+-----+----+\n |      \n |      Fill all null values with ``False`` for boolean columns.\n |      \n |      >>> df.na.fill(False).show()\n |      +----+------+-----+-----+\n |      | age|height| name| bool|\n |      +----+------+-----+-----+\n |      |  10|  80.5|Alice|false|\n |      |   5|  NULL|  Bob|false|\n |      |NULL|  NULL|  Tom|false|\n |      |NULL|  NULL| NULL| true|\n |      +----+------+-----+-----+\n |      \n |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n |      \n |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n |      +---+------+-------+----+\n |      |age|height|   name|bool|\n |      +---+------+-------+----+\n |      | 10|  80.5|  Alice|NULL|\n |      |  5|  NULL|    Bob|NULL|\n |      | 50|  NULL|    Tom|NULL|\n |      | 50|  NULL|unknown|true|\n |      +---+------+-------+----+\n |  \n |  replace(self, to_replace: Union[List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame\n |      Returns a new :class:`DataFrame` replacing a value with another value.\n |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n |      aliases of each other.\n |      Values to_replace and value must have the same type and can only be numerics, booleans,\n |      or strings. Value can have None. When replacing, the new value will be cast\n |      to the type of the existing column.\n |      For numeric replacements all values to be replaced should have unique\n |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n |      and arbitrary replacement will be used.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Supports Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      to_replace : bool, int, float, string, list or dict\n |          Value to be replaced.\n |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n |          must be a mapping between a value and a replacement.\n |      value : bool, int, float, string or None, optional\n |          The replacement value must be a bool, int, float, string or None. If `value` is a\n |          list, `value` should be of the same length and type as `to_replace`.\n |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n |          used as a replacement for each item in `to_replace`.\n |      subset : list, optional\n |          optional list of column names to consider.\n |          Columns specified in subset that do not have matching data types are ignored.\n |          For example, if `value` is a string, and subset contains a non-string column,\n |          then the non-string column is simply ignored.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |          DataFrame with replaced values.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([\n |      ...     (10, 80, \"Alice\"),\n |      ...     (5, None, \"Bob\"),\n |      ...     (None, 10, \"Tom\"),\n |      ...     (None, None, None)],\n |      ...     schema=[\"age\", \"height\", \"name\"])\n |      \n |      Replace 10 to 20 in all columns.\n |      \n |      >>> df.na.replace(10, 20).show()\n |      +----+------+-----+\n |      | age|height| name|\n |      +----+------+-----+\n |      |  20|    80|Alice|\n |      |   5|  NULL|  Bob|\n |      |NULL|    20|  Tom|\n |      |NULL|  NULL| NULL|\n |      +----+------+-----+\n |      \n |      Replace 'Alice' to null in all columns.\n |      \n |      >>> df.na.replace('Alice', None).show()\n |      +----+------+----+\n |      | age|height|name|\n |      +----+------+----+\n |      |  10|    80|NULL|\n |      |   5|  NULL| Bob|\n |      |NULL|    10| Tom|\n |      |NULL|  NULL|NULL|\n |      +----+------+----+\n |      \n |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n |      \n |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n |      +----+------+----+\n |      | age|height|name|\n |      +----+------+----+\n |      |  10|    80|   A|\n |      |   5|  NULL|   B|\n |      |NULL|    10| Tom|\n |      |NULL|  NULL|NULL|\n |      +----+------+----+\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(empdf.na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7628f406-2ee6-49bf-989d-45d41d1a75a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fill in module pyspark.sql.dataframe:\n\nfill(value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.dataframe.DataFrameNaFunctions instance\n    Replace null values, alias for ``na.fill()``.\n    :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n    \n    .. versionadded:: 1.3.1\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    value : int, float, string, bool or dict\n        Value to replace null values with.\n        If the value is a dict, then `subset` is ignored and `value` must be a mapping\n        from column name (string) to replacement value. The replacement value must be\n        an int, float, boolean, or string.\n    subset : str, tuple or list, optional\n        optional list of column names to consider.\n        Columns specified in subset that do not have matching data types are ignored.\n        For example, if `value` is a string, and subset contains a non-string column,\n        then the non-string column is simply ignored.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with replaced null values.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([\n    ...     (10, 80.5, \"Alice\", None),\n    ...     (5, None, \"Bob\", None),\n    ...     (None, None, \"Tom\", None),\n    ...     (None, None, None, True)],\n    ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n    \n    Fill all null values with 50 for numeric columns.\n    \n    >>> df.na.fill(50).show()\n    +---+------+-----+----+\n    |age|height| name|bool|\n    +---+------+-----+----+\n    | 10|  80.5|Alice|NULL|\n    |  5|  50.0|  Bob|NULL|\n    | 50|  50.0|  Tom|NULL|\n    | 50|  50.0| NULL|true|\n    +---+------+-----+----+\n    \n    Fill all null values with ``False`` for boolean columns.\n    \n    >>> df.na.fill(False).show()\n    +----+------+-----+-----+\n    | age|height| name| bool|\n    +----+------+-----+-----+\n    |  10|  80.5|Alice|false|\n    |   5|  NULL|  Bob|false|\n    |NULL|  NULL|  Tom|false|\n    |NULL|  NULL| NULL| true|\n    +----+------+-----+-----+\n    \n    Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n    \n    >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n    +---+------+-------+----+\n    |age|height|   name|bool|\n    +---+------+-------+----+\n    | 10|  80.5|  Alice|NULL|\n    |  5|  NULL|    Bob|NULL|\n    | 50|  NULL|    Tom|NULL|\n    | 50|  NULL|unknown|true|\n    +---+------+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(empdf.na.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89b61af-b144-437c-8dfd-09153ff8969e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fillna in module pyspark.sql.dataframe:\n\nfillna(value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Replace null values, alias for ``na.fill()``.\n    :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n    \n    .. versionadded:: 1.3.1\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    value : int, float, string, bool or dict\n        Value to replace null values with.\n        If the value is a dict, then `subset` is ignored and `value` must be a mapping\n        from column name (string) to replacement value. The replacement value must be\n        an int, float, boolean, or string.\n    subset : str, tuple or list, optional\n        optional list of column names to consider.\n        Columns specified in subset that do not have matching data types are ignored.\n        For example, if `value` is a string, and subset contains a non-string column,\n        then the non-string column is simply ignored.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with replaced null values.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([\n    ...     (10, 80.5, \"Alice\", None),\n    ...     (5, None, \"Bob\", None),\n    ...     (None, None, \"Tom\", None),\n    ...     (None, None, None, True)],\n    ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n    \n    Fill all null values with 50 for numeric columns.\n    \n    >>> df.na.fill(50).show()\n    +---+------+-----+----+\n    |age|height| name|bool|\n    +---+------+-----+----+\n    | 10|  80.5|Alice|NULL|\n    |  5|  50.0|  Bob|NULL|\n    | 50|  50.0|  Tom|NULL|\n    | 50|  50.0| NULL|true|\n    +---+------+-----+----+\n    \n    Fill all null values with ``False`` for boolean columns.\n    \n    >>> df.na.fill(False).show()\n    +----+------+-----+-----+\n    | age|height| name| bool|\n    +----+------+-----+-----+\n    |  10|  80.5|Alice|false|\n    |   5|  NULL|  Bob|false|\n    |NULL|  NULL|  Tom|false|\n    |NULL|  NULL| NULL| true|\n    +----+------+-----+-----+\n    \n    Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n    \n    >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n    +---+------+-------+----+\n    |age|height|   name|bool|\n    +---+------+-------+----+\n    | 10|  80.5|  Alice|NULL|\n    |  5|  NULL|    Bob|NULL|\n    | 50|  NULL|    Tom|NULL|\n    | 50|  NULL|unknown|true|\n    +---+------+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(empdf.fillna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a048b7dc-fedb-4824-9f65-80660585108a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|     NULL|1000.0|   30| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|\n|          3|      Nick|     NULL|  NULL|     |united kingdom| +44 111 111 111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1419d5-cf9e-4c2e-974e-94f1b8a6b552",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|     NULL|1000.0|   30| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|\n|          3|      Nick|     NULL|   0.0|     |united kingdom| +44 111 111 111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.fillna(0.0).show() # since 0.0 is float it fills with that value only in float columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdefe9ec-d5a0-462a-a1ce-bb589195c796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|       na|1000.0|   30| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|   na|         india|+91 234 567 8901|456 78 9123|\n|          3|      Nick|       na|  NULL|     |united kingdom| +44 111 111 111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.fillna(\"na\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "928d4ed6-38fe-430c-bbc8-744b2a3805a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|       na|1000.0|   30| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0|   na|         india|+91 234 567 8901|456 78 9123|\n|          3|      Nick|       na|   0.0|     |united kingdom| +44 111 111 111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.fillna(0.0).fillna(\"na\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa31968-eaed-4a75-bd3a-26d8951fa292",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n|          1|     Scott|       na|1000.0|   30| united states| +1 123 456 7890|123 45 6789|\n|          2|     Henry|     Ford|1250.0| NULL|         india|+91 234 567 8901|456 78 9123|\n|          3|      Nick|       na|   0.0|     |united kingdom| +44 111 111 111|222 33 4444|\n|          4|      Bill|    Gomes|1500.0|  150|     australia|+61 987 654 3210|789 12 6113|\n+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "empdf.fillna(0.0, 'salary').fillna(\"na\", 'last_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57595d2e-145b-4e87-8334-eb4138f7f6f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "19 - Dealing with nulls in Spark Data Frame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

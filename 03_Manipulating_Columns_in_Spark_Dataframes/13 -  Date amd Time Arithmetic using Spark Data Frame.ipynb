{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab37ec8-e29e-4a98-b2b3-62ac670122d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f19c69-71e2-4afb-bb6d-08d962a049d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [\n",
    "    (\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "    (\"2016-03-29\", \"2016-03-29 11:23:00.234\"),\n",
    "    (\"2018-04-20\", \"2018-04-20 12:34:00.543\"),\n",
    "    (\"2019-05-12\", \"2019-05-12 13:21:00.567\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1c3de4-ed38-4a33-a0ac-d55b9293c9df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datedf = spark.createDataFrame(datetimes, schema = 'date STRING, time STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732cd1cc-ef2b-4908-b5ec-bc1210f61c88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n|date      |time                   |\n+----------+-----------------------+\n|2014-02-28|2014-02-28 10:00:00.123|\n|2016-03-29|2016-03-29 11:23:00.234|\n|2018-04-20|2018-04-20 12:34:00.543|\n|2019-05-12|2019-05-12 13:21:00.567|\n+----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7aae85f-6139-4333-aedd-13721cf9b444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_add in module pyspark.sql.functions:\n\ndate_add(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n    Returns the date that is `days` days after `start`. If `days` is a negative value\n    then these amount of days will be deducted from `start`.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    start : :class:`~pyspark.sql.Column` or str\n        date column to work on.\n    days : :class:`~pyspark.sql.Column` or str or int\n        how many days after the given date to calculate.\n        Accepts negative value as well to calculate backwards in time.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        a date after/before given number of days.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'add'])\n    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n    [Row(next_date=datetime.date(2015, 4, 9))]\n    >>> df.select(date_add(df.dt, df.add.cast('integer')).alias('next_date')).collect()\n    [Row(next_date=datetime.date(2015, 4, 10))]\n    >>> df.select(date_add('dt', -1).alias('prev_date')).collect()\n    [Row(prev_date=datetime.date(2015, 4, 7))]\n\n"
     ]
    }
   ],
   "source": [
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd999902-21fb-4675-bdb1-9f790d173e16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_sub in module pyspark.sql.functions:\n\ndate_sub(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n    Returns the date that is `days` days before `start`. If `days` is a negative value\n    then these amount of days will be added to `start`.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    start : :class:`~pyspark.sql.Column` or str\n        date column to work on.\n    days : :class:`~pyspark.sql.Column` or str or int\n        how many days before the given date to calculate.\n        Accepts negative value as well to calculate forward in time.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        a date before/after given number of days.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'sub'])\n    >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n    [Row(prev_date=datetime.date(2015, 4, 7))]\n    >>> df.select(date_sub(df.dt, df.sub.cast('integer')).alias('prev_date')).collect()\n    [Row(prev_date=datetime.date(2015, 4, 6))]\n    >>> df.select(date_sub('dt', -1).alias('next_date')).collect()\n    [Row(next_date=datetime.date(2015, 4, 9))]\n\n"
     ]
    }
   ],
   "source": [
    "help(date_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0f5043-f35e-4260-8357-304fa93a6727",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n|date      |time                   |date_add_date|date_add_time|date_sub_date|date_sub_time|\n+----------+-----------------------+-------------+-------------+-------------+-------------+\n|2014-02-28|2014-02-28 10:00:00.123|2014-03-10   |2014-03-10   |2014-02-18   |2014-02-18   |\n|2016-03-29|2016-03-29 11:23:00.234|2016-04-08   |2016-04-08   |2016-03-19   |2016-03-19   |\n|2018-04-20|2018-04-20 12:34:00.543|2018-04-30   |2018-04-30   |2018-04-10   |2018-04-10   |\n|2019-05-12|2019-05-12 13:21:00.567|2019-05-22   |2019-05-22   |2019-05-02   |2019-05-02   |\n+----------+-----------------------+-------------+-------------+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf.\\\n",
    "    withColumn(\"date_add_date\", date_add(\"date\", 10)). \\\n",
    "    withColumn(\"date_add_time\", date_add(\"time\", 10)). \\\n",
    "    withColumn(\"date_sub_date\", date_sub(\"date\",10)). \\\n",
    "    withColumn(\"date_sub_time\", date_sub(\"time\", 10)). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9873fa-6575-44e4-b82c-93d88a477d46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_diff in module pyspark.sql.functions:\n\ndate_diff(end: 'ColumnOrName', start: 'ColumnOrName') -> pyspark.sql.column.Column\n    Returns the number of days from `start` to `end`.\n    \n    .. versionadded:: 3.5.0\n    \n    Parameters\n    ----------\n    end : :class:`~pyspark.sql.Column` or str\n        to date column to work on.\n    start : :class:`~pyspark.sql.Column` or str\n        from date column to work on.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        difference in days between two dates.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n    >>> df.select(date_diff(df.d2, df.d1).alias('diff')).collect()\n    [Row(diff=32)]\n\n"
     ]
    }
   ],
   "source": [
    "help(date_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b55f8a0-f877-444d-8e6b-bb5b32e3db04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n|date      |time                   |\n+----------+-----------------------+\n|2014-02-28|2014-02-28 10:00:00.123|\n|2016-03-29|2016-03-29 11:23:00.234|\n|2018-04-20|2018-04-20 12:34:00.543|\n|2019-05-12|2019-05-12 13:21:00.567|\n+----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b78b193-15b3-4b5f-a8ea-faff4b6837fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+\n|date      |time                   |datediff_date|datediff_time|\n+----------+-----------------------+-------------+-------------+\n|2014-02-28|2014-02-28 10:00:00.123|3509         |3509         |\n|2016-03-29|2016-03-29 11:23:00.234|2749         |2749         |\n|2018-04-20|2018-04-20 12:34:00.543|1997         |1997         |\n|2019-05-12|2019-05-12 13:21:00.567|1610         |1610         |\n+----------+-----------------------+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"datediff_date\", date_diff(current_date(), \"date\")). \\\n",
    "    withColumn(\"datediff_time\", date_diff(current_timestamp(), \"time\")). \\\n",
    "    show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75f0229-6d0c-46bb-99a8-e34ace057c03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function months_between in module pyspark.sql.functions:\n\nmonths_between(date1: 'ColumnOrName', date2: 'ColumnOrName', roundOff: bool = True) -> pyspark.sql.column.Column\n    Returns number of months between dates date1 and date2.\n    If date1 is later than date2, then the result is positive.\n    A whole number is returned if both inputs have the same day of month or both are the last day\n    of their respective months. Otherwise, the difference is calculated assuming 31 days per month.\n    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    date1 : :class:`~pyspark.sql.Column` or str\n        first date column.\n    date2 : :class:`~pyspark.sql.Column` or str\n        second date column.\n    roundOff : bool, optional\n        whether to round (to 8 digits) the final value or not (default: True).\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        number of months between two dates.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n    [Row(months=3.94959677)]\n    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n    [Row(months=3.9495967741935485)]\n\n"
     ]
    }
   ],
   "source": [
    "help(months_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa6fd0c-8813-4f8c-9492-a0566bc67830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function add_months in module pyspark.sql.functions:\n\nadd_months(start: 'ColumnOrName', months: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n    Returns the date that is `months` months after `start`. If `months` is a negative value\n    then these amount of months will be deducted from the `start`.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    start : :class:`~pyspark.sql.Column` or str\n        date column to work on.\n    months : :class:`~pyspark.sql.Column` or str or int\n        how many months after the given date to calculate.\n        Accepts negative value as well to calculate backwards.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        a date after/before given number of months.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('2015-04-08', 2)], ['dt', 'add'])\n    >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n    [Row(next_month=datetime.date(2015, 5, 8))]\n    >>> df.select(add_months(df.dt, df.add.cast('integer')).alias('next_month')).collect()\n    [Row(next_month=datetime.date(2015, 6, 8))]\n    >>> df.select(add_months('dt', -2).alias('prev_month')).collect()\n    [Row(prev_month=datetime.date(2015, 2, 8))]\n\n"
     ]
    }
   ],
   "source": [
    "help(add_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d11b80b5-3336-45e2-bd1b-3cdb1722214c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "most of date functions return date inspite of using it on timetsamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1050f90f-c384-4d2b-84fe-6a1bbffd1bf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n|2014-02-28|2014-02-28 10:00:00.123|115.35             |115.35             |2014-05-28     |2014-05-28     |\n|2016-03-29|2016-03-29 11:23:00.234|90.32              |90.32              |2016-06-29     |2016-06-29     |\n|2018-04-20|2018-04-20 12:34:00.543|65.61              |65.6               |2018-07-20     |2018-07-20     |\n|2019-05-12|2019-05-12 13:21:00.567|52.87              |52.86              |2019-08-12     |2019-08-12     |\n+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"months_between_date\", round(months_between(current_date(), \"date\"),2)). \\\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp(), \"time\"),2)). \\\n",
    "    withColumn(\"add_months_date\", add_months(\"date\", 3)). \\\n",
    "    withColumn(\"add_months_time\", add_months(\"time\",3)). \\\n",
    "    show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8fef17b-2eeb-472a-8436-975bb1c29025",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13 -  Date amd Time Arithmetic using Spark Data Frame",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

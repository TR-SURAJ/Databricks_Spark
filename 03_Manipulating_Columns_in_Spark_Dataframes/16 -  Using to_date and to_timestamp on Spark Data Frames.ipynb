{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd58a7f-400f-41ca-a665-05638f8ced00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e55db14-f1fd-4901-a2a5-a66a7af887cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [\n",
    "    (\"20140228\", \"28-Feb-2014 10:00:00.123\"),\n",
    "    (\"20160329\", \"29-Mar-2016 11:23:00.234\"),\n",
    "    (\"20180420\", \"20-Apr-2018 12:34:00.543\"),\n",
    "    (\"20190512\", \"12-May-2019 13:21:00.567\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8174a3ec-f7c1-4b39-b5df-cc30a9f8d744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datedf = spark.createDataFrame(datetimes, schema = 'date STRING, time STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b6c9e0-b478-4f86-bc14-7a71c95b01c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n|date    |time                    |\n+--------+------------------------+\n|20140228|28-Feb-2014 10:00:00.123|\n|20160329|29-Mar-2016 11:23:00.234|\n|20180420|20-Apr-2018 12:34:00.543|\n|20190512|12-May-2019 13:21:00.567|\n+--------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f23ef9-9723-4c62-bcbc-443527e4f088",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n\nto_date(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n    using the optionally specified format. Specify formats according to `datetime pattern`_.\n    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n    is omitted. Equivalent to ``col.cast(\"date\")``.\n    \n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n    \n    .. versionadded:: 2.2.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    col : :class:`~pyspark.sql.Column` or str\n        input column of values to convert.\n    format: str, optional\n        format to use to convert date values.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        date value as :class:`pyspark.sql.types.DateType` type.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_date(df.t).alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n    \n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n\n"
     ]
    }
   ],
   "source": [
    "help(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57058373-9765-4bd6-83fb-f14be789ba1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3e1aa5-fa97-43d6-8db8-ea9b26da0e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l, schema = \"dummy string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47477040-2acb-4514-9d41-4353ac0c45d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|dummy|\n+-----+\n|    X|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78bbb856-ba0f-4ed0-8ab7-dee0748d2d13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|   to_date|\n+----------+\n|2023-03-02|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('20230302'), 'yyyyMMdd').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd62b49e-16fe-4009-af41-6f55d5d985ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('2021061'), 'yyyyDDD').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efaa986d-2aae-4374-a4ef-7b3d05bb2ade",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('02/03/2021'), 'dd/MM/yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ffbfae-7ead-49a1-94fc-7ec806c73f17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('02-03-2021'), 'dd-MM-yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e2484cd-fd19-48cf-ad20-b7a6d6049eae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('02-Mar-2021'), 'dd-MMM-yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b5e245-411d-4056-b4ee-be35d7ad8c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|   to_date|\n+----------+\n|2021-03-02|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('March 2, 2021'), 'MMMM d, yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314fd1d4-2ff7-478c-b4bf-95d6413b7043",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_timestamp in module pyspark.sql.functions:\n\nto_timestamp(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n    using the optionally specified format. Specify formats according to `datetime pattern`_.\n    By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n    is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n    \n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n    \n    .. versionadded:: 2.2.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    col : :class:`~pyspark.sql.Column` or str\n        column values to convert.\n    format: str, optional\n        format to use to convert timestamp values.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        timestamp value as :class:`pyspark.sql.types.TimestampType` type.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n    \n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n\n"
     ]
    }
   ],
   "source": [
    "help(to_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ebe1a0-88c4-4bc5-8892-6c288e45b5ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|            to_date|\n+-------------------+\n|2021-03-02 00:00:00|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('02-Mar-2021'), 'dd-MMM-yyyy').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727051b6-3947-49c0-bd64-8d9da3324af6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|            to_date|\n+-------------------+\n|2021-03-02 17:30:15|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('02-Mar-2021 17:30:15'), 'dd-MMM-yyyy HH:mm:ss').alias('to_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8075cb27-dce2-4278-88d8-38e3071b97ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- date: string (nullable = true)\n |-- time: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "datedf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d6fd92-64b8-403d-a07b-02064f9d72d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n|date    |time                    |\n+--------+------------------------+\n|20140228|28-Feb-2014 10:00:00.123|\n|20160329|29-Mar-2016 11:23:00.234|\n|20180420|20-Apr-2018 12:34:00.543|\n|20190512|12-May-2019 13:21:00.567|\n+--------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fa7f3fc-39e3-40f5-a983-d9f28adbfd5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------+-----------------------+\n|date    |time                    |to_date   |to_timestamp           |\n+--------+------------------------+----------+-----------------------+\n|20140228|28-Feb-2014 10:00:00.123|2014-02-28|2014-02-28 10:00:00.123|\n|20160329|29-Mar-2016 11:23:00.234|2016-03-29|2016-03-29 11:23:00.234|\n|20180420|20-Apr-2018 12:34:00.543|2018-04-20|2018-04-20 12:34:00.543|\n|20190512|12-May-2019 13:21:00.567|2019-05-12|2019-05-12 13:21:00.567|\n+--------+------------------------+----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"to_date\", to_date('date', 'yyyyMMdd')). \\\n",
    "    withColumn(\"to_timestamp\", to_timestamp(col('time'), 'dd-MMM-yyyy HH:mm:ss.SSS')).\\\n",
    "    show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ea95fa-d522-4b33-81a8-334f15ee6ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "16 -  Using to_date and to_timestamp on Spark Data Frames",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

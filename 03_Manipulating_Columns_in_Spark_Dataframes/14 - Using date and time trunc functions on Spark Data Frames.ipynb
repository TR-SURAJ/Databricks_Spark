{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b31350e-2636-4f1e-afc4-b9600857e862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c88a17-a0b1-4d84-b159-a6cd25056c41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [\n",
    "    (\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "    (\"2016-03-29\", \"2016-03-29 11:23:00.234\"),\n",
    "    (\"2018-04-20\", \"2018-04-20 12:34:00.543\"),\n",
    "    (\"2019-05-12\", \"2019-05-12 13:21:00.567\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b544fd02-8f9e-4a1b-82f3-fd99f750f0af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datedf = spark.createDataFrame(datetimes, schema = 'date STRING, time STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12757787-50cc-4323-bf8f-570abd4697d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n|date      |time                   |\n+----------+-----------------------+\n|2014-02-28|2014-02-28 10:00:00.123|\n|2016-03-29|2016-03-29 11:23:00.234|\n|2018-04-20|2018-04-20 12:34:00.543|\n|2019-05-12|2019-05-12 13:21:00.567|\n+----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6300cb-1eca-43f9-9ca6-e067d73aabe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trunc in module pyspark.sql.functions:\n\ntrunc(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n    Returns date truncated to the unit specified by the format.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    date : :class:`~pyspark.sql.Column` or str\n        input column of values to truncate.\n    format : str\n        'year', 'yyyy', 'yy' to truncate by year,\n        or 'month', 'mon', 'mm' to truncate by month\n        Other options are: 'week', 'quarter'\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        truncated date.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n    >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n    [Row(year=datetime.date(1997, 1, 1))]\n    >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n    [Row(month=datetime.date(1997, 2, 1))]\n\n"
     ]
    }
   ],
   "source": [
    "help(trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf9a9cb-2201-4328-a0ce-aa27ed319169",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_trunc in module pyspark.sql.functions:\n\ndate_trunc(format: str, timestamp: 'ColumnOrName') -> pyspark.sql.column.Column\n    Returns timestamp truncated to the unit specified by the format.\n    \n    .. versionadded:: 2.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    format : str\n        'year', 'yyyy', 'yy' to truncate by year,\n        'month', 'mon', 'mm' to truncate by month,\n        'day', 'dd' to truncate by day,\n        Other options are:\n        'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n    timestamp : :class:`~pyspark.sql.Column` or str\n        input column of values to truncate.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        truncated timestamp.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n\n"
     ]
    }
   ],
   "source": [
    "help(date_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47e131e8-8a70-408e-9f30-b66d3621ba43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "-  Get beginning month date using date field and beginning year date using time field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91aef8e-7220-4738-92ec-3c436b2f4fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+------------------+\n|date      |time                   |date_trunc_by_month|time_trunc_by_year|\n+----------+-----------------------+-------------------+------------------+\n|2014-02-28|2014-02-28 10:00:00.123|2014-02-01         |2014-01-01        |\n|2016-03-29|2016-03-29 11:23:00.234|2016-03-01         |2016-01-01        |\n|2018-04-20|2018-04-20 12:34:00.543|2018-04-01         |2018-01-01        |\n|2019-05-12|2019-05-12 13:21:00.567|2019-05-01         |2019-01-01        |\n+----------+-----------------------+-------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"date_trunc_by_month\", trunc(\"date\",\"MM\")). \\\n",
    "    withColumn(\"time_trunc_by_year\", trunc(\"time\", \"yy\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "726684ec-d974-417e-a79b-21c256b4b29b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Get beginning hour time using date and time field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3167f53a-53b1-4bc5-98e2-35875c3f2dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_trunc in module pyspark.sql.functions:\n\ndate_trunc(format: str, timestamp: 'ColumnOrName') -> pyspark.sql.column.Column\n    Returns timestamp truncated to the unit specified by the format.\n    \n    .. versionadded:: 2.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    format : str\n        'year', 'yyyy', 'yy' to truncate by year,\n        'month', 'mon', 'mm' to truncate by month,\n        'day', 'dd' to truncate by day,\n        Other options are:\n        'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n    timestamp : :class:`~pyspark.sql.Column` or str\n        input column of values to truncate.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        truncated timestamp.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n\n"
     ]
    }
   ],
   "source": [
    "help(date_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c6741c-c93b-4caf-b624-c4b05a5cc2fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+\n|date      |time                   |date_trunc_by_month|time_trunc_by_year |\n+----------+-----------------------+-------------------+-------------------+\n|2014-02-28|2014-02-28 10:00:00.123|2014-02-01 00:00:00|2014-01-01 00:00:00|\n|2016-03-29|2016-03-29 11:23:00.234|2016-03-01 00:00:00|2016-01-01 00:00:00|\n|2018-04-20|2018-04-20 12:34:00.543|2018-04-01 00:00:00|2018-01-01 00:00:00|\n|2019-05-12|2019-05-12 13:21:00.567|2019-05-01 00:00:00|2019-01-01 00:00:00|\n+----------+-----------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"date_trunc_by_month\", date_trunc('MM', \"date\")). \\\n",
    "    withColumn(\"time_trunc_by_year\", date_trunc('yy', \"time\")). \\\n",
    "    show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce31ebc-53f6-404a-a4bc-d909aeab3805",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n|date      |time                   |date_dt            |time_dt            |time_dt1           |\n+----------+-----------------------+-------------------+-------------------+-------------------+\n|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|2014-02-28 00:00:00|\n|2016-03-29|2016-03-29 11:23:00.234|2016-03-29 00:00:00|2016-03-29 11:00:00|2016-03-29 00:00:00|\n|2018-04-20|2018-04-20 12:34:00.543|2018-04-20 00:00:00|2018-04-20 12:00:00|2018-04-20 00:00:00|\n|2019-05-12|2019-05-12 13:21:00.567|2019-05-12 00:00:00|2019-05-12 13:00:00|2019-05-12 00:00:00|\n+----------+-----------------------+-------------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"date_dt\", date_trunc(\"HOUR\", \"date\")). \\\n",
    "    withColumn(\"time_dt\", date_trunc(\"HOUR\", \"time\")). \\\n",
    "    withColumn(\"time_dt1\", date_trunc(\"dd\", \"time\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "320c96f6-0a37-45d2-a492-53d2c22c3f8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "14 - Using date and time trunc functions on Spark Data Frames",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

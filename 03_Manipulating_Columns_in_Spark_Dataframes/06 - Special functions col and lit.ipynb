{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ed4637-5082-4cf4-9bf1-779cbef599f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_list = [(1,\"bob\",\"jackson\",1500.0,\"AUSTRALIA\",\"+61 9890989789\",\"345 23 5645\"),\n",
    "            (2,\"hill\",\"martin\",750.0,\"UK\",\"+44 3465789709\",\"390 45 7598\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01539f62-c979-479f-a948-fda1d3614116",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp = spark.createDataFrame(employee_list, schema = \"\"\"employee_id INT, first_name STRING, last_name STRING, salary FLOAT, nationality STRING,\n",
    "                            phone_number STRING, ssn STRING \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddea234d-06d1-4ab6-94be-3a47994207a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f3f19b-5724-4bce-83b3-16e2f132e033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c403307-ea8e-4d24-9b5a-09eec2967b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|nationality|count|\n+-----------+-----+\n|  AUSTRALIA|    1|\n|         UK|    1|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(\"nationality\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2220e3f5-161c-4937-9d44-57576c52fac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.orderBy(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b44edc11-7aae-4b9c-8fab-1429c35d0c0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- If there are no transformations on any column in any function then we shold be able to pass all column names as string\n",
    "\n",
    "- If we have to invoke functions such as cast,desc, etc to customize the behaviour then we have to build column object using col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90efa4de-4469-4e42-861e-e590f12b7ea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function col in module pyspark.sql.functions:\n\ncol(col: str) -> pyspark.sql.column.Column\n    Returns a :class:`~pyspark.sql.Column` based on the given column name.\n    \n    Examples\n    --------\n    >>> col('x')\n    Column<'x'>\n    >>> column('x')\n    Column<'x'>\n    \n    .. versionadded:: 1.3\n\n"
     ]
    }
   ],
   "source": [
    "help(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b168f0a1-e61a-45d2-b24d-b1d43d2d3b86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: pyspark.sql.column.Column"
     ]
    }
   ],
   "source": [
    "type(col('first_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1d36b7-e058-4f3c-a329-b6b9f5ed97a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|first_name|last_name|\n+----------+---------+\n|       bob|  jackson|\n|      hill|   martin|\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.select(col('first_name'),col('last_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adff463c-7293-4757-a764-5303ba2888d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function upper in module pyspark.sql.functions:\n\nupper(col: 'ColumnOrName') -> pyspark.sql.column.Column\n    Converts a string expression to upper case.\n    \n    .. versionadded:: 1.5\n\n"
     ]
    }
   ],
   "source": [
    "help(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e84c25-25c5-43a1-be1f-6250ac51192f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: pyspark.sql.column.Column"
     ]
    }
   ],
   "source": [
    "type(upper('first_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c20a4ff-717d-4114-b4de-bcb9cc1ae370",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n|upper(first_name)|upper(last_name)|\n+-----------------+----------------+\n|              BOB|         JACKSON|\n|             HILL|          MARTIN|\n+-----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.select(upper(\"first_name\"),upper(\"last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf1c8ad4-2d21-4983-a574-62a14f8739b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n|upper(first_name)|upper(last_name)|\n+-----------------+----------------+\n|              BOB|         JACKSON|\n|             HILL|          MARTIN|\n+-----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.select(upper(col(\"first_name\")),upper(col(\"last_name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e6bb4c-1edd-49af-b340-e87f51df5914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|nationality|count|\n+-----------+-----+\n|  AUSTRALIA|    1|\n|         UK|    1|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp.groupBy(upper(\"nationality\").alias('nationality')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6760aeb2-d5b1-4e8c-a4d5-d6ea27216546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Column in module pyspark.sql.column:\n\nclass Column(builtins.object)\n |  Column(jc: py4j.java_gateway.JavaObject) -> None\n |  \n |  A column in a DataFrame.\n |  \n |  :class:`Column` instances can be created by::\n |  \n |      # 1. Select a column out of a DataFrame\n |  \n |      df.colName\n |      df[\"colName\"]\n |  \n |      # 2. Create from an expression\n |      df.colName + 1\n |      1 / df.colName\n |  \n |  .. versionadded:: 1.3.0\n |  \n |  Methods defined here:\n |  \n |  __add__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __and__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __bool__ = __nonzero__(self) -> None\n |  \n |  __contains__(self, item: Any) -> None\n |      # container operators\n |  \n |  __div__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __eq__(self, other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary function\n |  \n |  __ge__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __getattr__(self, item: Any) -> 'Column'\n |  \n |  __getitem__(self, k: Any) -> 'Column'\n |  \n |  __gt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __init__(self, jc: py4j.java_gateway.JavaObject) -> None\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __invert__ = _(self: 'Column') -> 'Column'\n |  \n |  __iter__(self) -> None\n |  \n |  __le__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __lt__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __mod__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __mul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __ne__(self, other: Any) -> 'Column'\n |      binary function\n |  \n |  __neg__ = _(self: 'Column') -> 'Column'\n |  \n |  __nonzero__(self) -> None\n |  \n |  __or__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __pow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n |      binary function\n |  \n |  __radd__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __rand__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __rdiv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  __rmod__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __rmul__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __ror__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __rpow__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n |      binary function\n |  \n |  __rsub__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __rtruediv__ = _(self: 'Column', other: Union[ForwardRef('LiteralType'), ForwardRef('DecimalLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __sub__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  __truediv__ = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      binary operator\n |  \n |  alias(self, *alias: str, **kwargs: Any) -> 'Column'\n |      Returns this column aliased with a new name or names (in the case of expressions that\n |      return more than one column, such as explode).\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      alias : str\n |          desired column names (collects all positional arguments passed)\n |      \n |      Other Parameters\n |      ----------------\n |      metadata: dict\n |          a dict of information to be stored in ``metadata`` attribute of the\n |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n |          only argument)\n |      \n |          .. versionchanged:: 2.2.0\n |             Added optional ``metadata`` argument.\n |      \n |      Examples\n |      --------\n |      >>> df.select(df.age.alias(\"age2\")).collect()\n |      [Row(age2=2), Row(age2=5)]\n |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n |      99\n |  \n |  asc = _(self: 'Column') -> 'Column'\n |      Returns a sort expression based on ascending order of the column.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n |      [Row(name='Alice'), Row(name='Tom')]\n |  \n |  asc_nulls_first = _(self: 'Column') -> 'Column'\n |      Returns a sort expression based on ascending order of the column, and null values\n |      return before non-null values.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n |  \n |  asc_nulls_last = _(self: 'Column') -> 'Column'\n |      Returns a sort expression based on ascending order of the column, and null values\n |      appear after non-null values.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n |  \n |  astype = cast(self, dataType)\n |      :func:`astype` is an alias for :func:`cast`.\n |      \n |      .. versionadded:: 1.4\n |  \n |  between(self, lowerBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')], upperBound: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DateTimeLiteral'), ForwardRef('DecimalLiteral')]) -> 'Column'\n |      True if the current column is between the lower bound and upper bound, inclusive.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.select(df.name, df.age.between(2, 4)).show()\n |      +-----+---------------------------+\n |      | name|((age >= 2) AND (age <= 4))|\n |      +-----+---------------------------+\n |      |Alice|                       true|\n |      |  Bob|                      false|\n |      +-----+---------------------------+\n |  \n |  bitwiseAND = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      Compute bitwise AND of this expression with another expression.\n |      \n |      Parameters\n |      ----------\n |      other\n |          a value or :class:`Column` to calculate bitwise and(&) with\n |          this :class:`Column`.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n |      [Row((a & b)=10)]\n |  \n |  bitwiseOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      Compute bitwise OR of this expression with another expression.\n |      \n |      Parameters\n |      ----------\n |      other\n |          a value or :class:`Column` to calculate bitwise or(|) with\n |          this :class:`Column`.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n |      [Row((a | b)=235)]\n |  \n |  bitwiseXOR = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      Compute bitwise XOR of this expression with another expression.\n |      \n |      Parameters\n |      ----------\n |      other\n |          a value or :class:`Column` to calculate bitwise xor(^) with\n |          this :class:`Column`.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n |      [Row((a ^ b)=225)]\n |  \n |  cast(self, dataType: Union[pyspark.sql.types.DataType, str]) -> 'Column'\n |      Casts the column into type ``dataType``.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n |      [Row(ages='2'), Row(ages='5')]\n |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n |      [Row(ages='2'), Row(ages='5')]\n |  \n |  contains = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n |      \n |      Parameters\n |      ----------\n |      other\n |          string in line. A value as a literal or a :class:`Column`.\n |      \n |      Examples\n |      --------\n |      >>> df.filter(df.name.contains('o')).collect()\n |      [Row(age=5, name='Bob')]\n |  \n |  desc = _(self: 'Column') -> 'Column'\n |      Returns a sort expression based on the descending order of the column.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n |      [Row(name='Tom'), Row(name='Alice')]\n |  \n |  desc_nulls_first = _(self: 'Column') -> 'Column'\n |      Returns a sort expression based on the descending order of the column, and null values\n |      appear before non-null values.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n |  \n |  desc_nulls_last = _(self: 'Column') -> 'Column'\n |      Returns a sort expression based on the descending order of the column, and null values\n |      appear after non-null values.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n |  \n |  dropFields(self, *fieldNames: str) -> 'Column'\n |      An expression that drops fields in :class:`StructType` by name.\n |      This is a no-op if schema doesn't contain field name(s).\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> from pyspark.sql.functions import col, lit\n |      >>> df = spark.createDataFrame([\n |      ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n |      >>> df.withColumn('a', df['a'].dropFields('b')).show()\n |      +-----------------+\n |      |                a|\n |      +-----------------+\n |      |{2, 3, {4, 5, 6}}|\n |      +-----------------+\n |      \n |      >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n |      +--------------+\n |      |             a|\n |      +--------------+\n |      |{3, {4, 5, 6}}|\n |      +--------------+\n |      \n |      This method supports dropping multiple nested fields directly e.g.\n |      \n |      >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n |      +--------------+\n |      |             a|\n |      +--------------+\n |      |{1, 2, 3, {4}}|\n |      +--------------+\n |      \n |      However, if you are going to add/replace multiple nested fields,\n |      it is preferred to extract out the nested struct before\n |      adding/replacing multiple fields e.g.\n |      \n |      >>> df.select(col(\"a\").withField(\n |      ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n |      ... ).show()\n |      +--------------+\n |      |             a|\n |      +--------------+\n |      |{1, 2, 3, {4}}|\n |      +--------------+\n |  \n |  endswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      String ends with. Returns a boolean :class:`Column` based on a string match.\n |      \n |      Parameters\n |      ----------\n |      other : :class:`Column` or str\n |          string at end of line (do not use a regex `$`)\n |      \n |      Examples\n |      --------\n |      >>> df.filter(df.name.endswith('ice')).collect()\n |      [Row(age=2, name='Alice')]\n |      >>> df.filter(df.name.endswith('ice$')).collect()\n |      []\n |  \n |  eqNullSafe = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      Equality test that is safe for null values.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Parameters\n |      ----------\n |      other\n |          a value or :class:`Column`\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df1 = spark.createDataFrame([\n |      ...     Row(id=1, value='foo'),\n |      ...     Row(id=2, value=None)\n |      ... ])\n |      >>> df1.select(\n |      ...     df1['value'] == 'foo',\n |      ...     df1['value'].eqNullSafe('foo'),\n |      ...     df1['value'].eqNullSafe(None)\n |      ... ).show()\n |      +-------------+---------------+----------------+\n |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n |      +-------------+---------------+----------------+\n |      |         true|           true|           false|\n |      |         null|          false|            true|\n |      +-------------+---------------+----------------+\n |      >>> df2 = spark.createDataFrame([\n |      ...     Row(value = 'bar'),\n |      ...     Row(value = None)\n |      ... ])\n |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n |      0\n |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n |      1\n |      >>> df2 = spark.createDataFrame([\n |      ...     Row(id=1, value=float('NaN')),\n |      ...     Row(id=2, value=42.0),\n |      ...     Row(id=3, value=None)\n |      ... ])\n |      >>> df2.select(\n |      ...     df2['value'].eqNullSafe(None),\n |      ...     df2['value'].eqNullSafe(float('NaN')),\n |      ...     df2['value'].eqNullSafe(42.0)\n |      ... ).show()\n |      +----------------+---------------+----------------+\n |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n |      +----------------+---------------+----------------+\n |      |           false|           true|           false|\n |      |           false|          false|            true|\n |      |            true|          false|           false|\n |      +----------------+---------------+----------------+\n |      \n |      Notes\n |      -----\n |      Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the\n |      `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_\n |      for details.\n |  \n |  getField(self, name: Any) -> 'Column'\n |      An expression that gets a field by name in a :class:`StructType`.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n |      >>> df.select(df.r.getField(\"b\")).show()\n |      +---+\n |      |r.b|\n |      +---+\n |      |  b|\n |      +---+\n |      >>> df.select(df.r.a).show()\n |      +---+\n |      |r.a|\n |      +---+\n |      |  1|\n |      +---+\n |  \n |  getItem(self, key: Any) -> 'Column'\n |      An expression that gets an item at position ``ordinal`` out of a list,\n |      or gets an item by key out of a dict.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Examples\n |      --------\n |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n |      +----+------+\n |      |l[0]|d[key]|\n |      +----+------+\n |      |   1| value|\n |      +----+------+\n |  \n |  ilike = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      SQL ILIKE expression (case insensitive LIKE). Returns a boolean :class:`Column`\n |      based on a case insensitive match.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      other : str\n |          a SQL LIKE pattern\n |      \n |      See Also\n |      --------\n |      pyspark.sql.Column.rlike\n |      \n |      Examples\n |      --------\n |      >>> df.filter(df.name.ilike('%Ice')).collect()\n |      [Row(age=2, name='Alice')]\n |  \n |  isNotNull = _(self: 'Column') -> 'Column'\n |      True if the current expression is NOT null.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n |      >>> df.filter(df.height.isNotNull()).collect()\n |      [Row(name='Tom', height=80)]\n |  \n |  isNull = _(self: 'Column') -> 'Column'\n |      True if the current expression is null.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n |      >>> df.filter(df.height.isNull()).collect()\n |      [Row(name='Alice', height=None)]\n |  \n |  isin(self, *cols: Any) -> 'Column'\n |      A boolean expression that is evaluated to true if the value of this\n |      expression is contained by the evaluated values of the arguments.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      Examples\n |      --------\n |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n |      [Row(age=5, name='Bob')]\n |      >>> df[df.age.isin([1, 2, 3])].collect()\n |      [Row(age=2, name='Alice')]\n |  \n |  like = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n |      \n |      Parameters\n |      ----------\n |      other : str\n |          a SQL LIKE pattern\n |      \n |      See Also\n |      --------\n |      pyspark.sql.Column.rlike\n |      \n |      Examples\n |      --------\n |      >>> df.filter(df.name.like('Al%')).collect()\n |      [Row(age=2, name='Alice')]\n |  \n |  name = alias(self, *alias, **kwargs)\n |      :func:`name` is an alias for :func:`alias`.\n |      \n |      .. versionadded:: 2.0\n |  \n |  otherwise(self, value: Any) -> 'Column'\n |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      value\n |          a literal value, or a :class:`Column` expression.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import functions as F\n |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n |      +-----+-------------------------------------+\n |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n |      +-----+-------------------------------------+\n |      |Alice|                                    0|\n |      |  Bob|                                    1|\n |      +-----+-------------------------------------+\n |      \n |      See Also\n |      --------\n |      pyspark.sql.functions.when\n |  \n |  over(self, window: 'WindowSpec') -> 'Column'\n |      Define a windowing column.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      window : :class:`WindowSpec`\n |      \n |      Returns\n |      -------\n |      :class:`Column`\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Window\n |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n |      >>> from pyspark.sql.functions import rank, min\n |      >>> from pyspark.sql.functions import desc\n |      >>> df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n |      +---+-----+----+---+\n |      |age| name|rank|min|\n |      +---+-----+----+---+\n |      |  5|  Bob|   1|  5|\n |      |  2|Alice|   1|  2|\n |      +---+-----+----+---+\n |  \n |  rlike = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n |      match.\n |      \n |      Parameters\n |      ----------\n |      other : str\n |          an extended regex expression\n |      \n |      Examples\n |      --------\n |      >>> df.filter(df.name.rlike('ice$')).collect()\n |      [Row(age=2, name='Alice')]\n |  \n |  startswith = _(self: 'Column', other: Union[ForwardRef('Column'), ForwardRef('LiteralType'), ForwardRef('DecimalLiteral'), ForwardRef('DateTimeLiteral')]) -> 'Column'\n |      String starts with. Returns a boolean :class:`Column` based on a string match.\n |      \n |      Parameters\n |      ----------\n |      other : :class:`Column` or str\n |          string at start of line (do not use a regex `^`)\n |      \n |      Examples\n |      --------\n |      >>> df.filter(df.name.startswith('Al')).collect()\n |      [Row(age=2, name='Alice')]\n |      >>> df.filter(df.name.startswith('^Al')).collect()\n |      []\n |  \n |  substr(self, startPos: Union[int, ForwardRef('Column')], length: Union[int, ForwardRef('Column')]) -> 'Column'\n |      Return a :class:`Column` which is a substring of the column.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      startPos : :class:`Column` or int\n |          start position\n |      length : :class:`Column` or int\n |          length of the substring\n |      \n |      Examples\n |      --------\n |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n |      [Row(col='Ali'), Row(col='Bob')]\n |  \n |  when(self, condition: 'Column', value: Any) -> 'Column'\n |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      condition : :class:`Column`\n |          a boolean :class:`Column` expression.\n |      value\n |          a literal value, or a :class:`Column` expression.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import functions as F\n |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n |      +-----+------------------------------------------------------------+\n |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n |      +-----+------------------------------------------------------------+\n |      |Alice|                                                          -1|\n |      |  Bob|                                                           1|\n |      +-----+------------------------------------------------------------+\n |      \n |      See Also\n |      --------\n |      pyspark.sql.functions.when\n |  \n |  withField(self, fieldName: str, col: 'Column') -> 'Column'\n |      An expression that adds/replaces a field in :class:`StructType` by name.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> from pyspark.sql.functions import lit\n |      >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n |      >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n |      +---+\n |      |  b|\n |      +---+\n |      |  3|\n |      +---+\n |      >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n |      +---+\n |      |  d|\n |      +---+\n |      |  4|\n |      +---+\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __hash__ = None\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.column import Column\n",
    "help(Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a32aa71-3aad-47e0-a345-2c478ea68dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "\u001B[0;32m<command-3025750883095265>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"employee_id\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdesc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'desc'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-3025750883095265>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"employee_id\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdesc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'desc'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'str' object has no attribute 'desc'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp.orderBy(\"employee_id\".desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af51089-ad63-4d97-a697-9e3a33f2e82c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.orderBy(col(\"employee_id\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c061f265-8a88-41b6-a6cb-0891cb94a0b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# we are not projecting the col, so its not changes to upeer case\n",
    "emp.orderBy(upper(emp['first_name']).alias('first_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbd4a85-9496-4105-9079-70d626b4925e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# we are not projecting the col, so its not changes to upeer case\n",
    "emp.orderBy(upper(emp.first_name).alias('first_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d92cae30-3fc6-461a-9950-6e2aa8e81b4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-3025750883095269>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'first_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\", \"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'last_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mselect\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2107\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Bob'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   2108\u001B[0m         \"\"\"\n",
       "\u001B[0;32m-> 2109\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jcols\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   2110\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   2111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `, ` cannot be resolved. Did you mean one of the following? [`ssn`, `salary`, `last_name`, `first_name`, `employee_id`];\n",
       "'Project [unresolvedalias(concat(first_name#3, ', , last_name#4), Some(org.apache.spark.sql.Column$$Lambda$7271/2067259968@6e33e124))]\n",
       "+- LogicalRDD [employee_id#2, first_name#3, last_name#4, salary#5, nationality#6, phone_number#7, ssn#8], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-3025750883095269>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'first_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\", \"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'last_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mselect\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2107\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Bob'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2108\u001B[0m         \"\"\"\n\u001B[0;32m-> 2109\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jcols\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2110\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `, ` cannot be resolved. Did you mean one of the following? [`ssn`, `salary`, `last_name`, `first_name`, `employee_id`];\n'Project [unresolvedalias(concat(first_name#3, ', , last_name#4), Some(org.apache.spark.sql.Column$$Lambda$7271/2067259968@6e33e124))]\n+- LogicalRDD [employee_id#2, first_name#3, last_name#4, salary#5, nationality#6, phone_number#7, ssn#8], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `, ` cannot be resolved. Did you mean one of the following? [`ssn`, `salary`, `last_name`, `first_name`, `employee_id`];\n'Project [unresolvedalias(concat(first_name#3, ', , last_name#4), Some(org.apache.spark.sql.Column$$Lambda$7271/2067259968@6e33e124))]\n+- LogicalRDD [employee_id#2, first_name#3, last_name#4, salary#5, nationality#6, phone_number#7, ssn#8], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp. \\\n",
    "    select(concat(col('first_name'),\", \",col('last_name'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ae950f-bb3b-4e85-a290-45a073f573d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-3025750883095270>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'first_name'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\", \"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0memp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'last_name'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mselect\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2107\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Bob'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   2108\u001B[0m         \"\"\"\n",
       "\u001B[0;32m-> 2109\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jcols\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   2110\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   2111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `, ` cannot be resolved. Did you mean one of the following? [`ssn`, `salary`, `last_name`, `first_name`, `employee_id`];\n",
       "'Project [unresolvedalias(concat(first_name#3, ', , last_name#4), Some(org.apache.spark.sql.Column$$Lambda$7271/2067259968@6e33e124))]\n",
       "+- LogicalRDD [employee_id#2, first_name#3, last_name#4, salary#5, nationality#6, phone_number#7, ssn#8], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-3025750883095270>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'first_name'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\", \"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0memp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'last_name'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mselect\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2107\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Bob'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2108\u001B[0m         \"\"\"\n\u001B[0;32m-> 2109\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jcols\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2110\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `, ` cannot be resolved. Did you mean one of the following? [`ssn`, `salary`, `last_name`, `first_name`, `employee_id`];\n'Project [unresolvedalias(concat(first_name#3, ', , last_name#4), Some(org.apache.spark.sql.Column$$Lambda$7271/2067259968@6e33e124))]\n+- LogicalRDD [employee_id#2, first_name#3, last_name#4, salary#5, nationality#6, phone_number#7, ssn#8], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `, ` cannot be resolved. Did you mean one of the following? [`ssn`, `salary`, `last_name`, `first_name`, `employee_id`];\n'Project [unresolvedalias(concat(first_name#3, ', , last_name#4), Some(org.apache.spark.sql.Column$$Lambda$7271/2067259968@6e33e124))]\n+- LogicalRDD [employee_id#2, first_name#3, last_name#4, salary#5, nationality#6, phone_number#7, ssn#8], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp. \\\n",
    "    select(concat(emp['first_name'],\", \",emp['last_name'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907b2d56-10ca-41cf-baff-fdd061fa94c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lit in module pyspark.sql.functions:\n\nlit(col: Any) -> pyspark.sql.column.Column\n    Creates a :class:`~pyspark.sql.Column` of literal value.\n    \n    .. versionadded:: 1.3.0\n    \n    Examples\n    --------\n    >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n    [Row(height=5, spark_user=True)]\n\n"
     ]
    }
   ],
   "source": [
    "help(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d645327f-c744-49a2-9b91-4dca62489a86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module pyspark.sql.functions:\n\nconcat(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n    Concatenates multiple input columns together into a single column.\n    The function works with strings, binary and compatible array columns.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat(df.s, df.d).alias('s')).collect()\n    [Row(s='abcd123')]\n    \n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n\n"
     ]
    }
   ],
   "source": [
    "help(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859d0bd8-3ea0-43e7-ac8a-3063063989ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+-----+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|bonus|\n+-----------+----------+---------+------+-----------+--------------+-----------+-----+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645| null|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598| null|\n+-----------+----------+---------+------+-----------+--------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# doesnt work, we have to use both col and lit \n",
    "\n",
    "emp.withColumn('bonus', 'salary' * lit(0.2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4eb8b6-3273-4e51-a912-b9702a1fb291",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+-----+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|bonus|\n+-----------+----------+---------+------+-----------+--------------+-----------+-----+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|300.0|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|150.0|\n+-----------+----------+---------+------+-----------+--------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "emp.withColumn('bonus', col('salary') * lit(0.2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17378728-714c-4ceb-b4b6-2c625c1b2c3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06 - Special functions col and lit",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

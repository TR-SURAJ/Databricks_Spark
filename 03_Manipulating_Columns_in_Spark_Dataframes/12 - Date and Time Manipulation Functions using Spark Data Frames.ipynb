{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ff2a9e-09ae-4532-88e3-5559ac238c0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bbf896-d4ce-4891-9732-d95c68616243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function current_timestamp in module pyspark.sql.functions:\n\ncurrent_timestamp() -> pyspark.sql.column.Column\n    Returns the current timestamp at the start of query evaluation as a :class:`TimestampType`\n    column. All calls of current_timestamp within the same query return the same value.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        current date and time.\n    \n    Examples\n    --------\n    >>> df = spark.range(1)\n    >>> df.select(current_timestamp()).show(truncate=False) # doctest: +SKIP\n    +-----------------------+\n    |current_timestamp()    |\n    +-----------------------+\n    |2022-08-26 21:23:22.716|\n    +-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "help(current_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d4839a-9033-430c-b412-814a7dc3e72d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function current_date in module pyspark.sql.functions:\n\ncurrent_date() -> pyspark.sql.column.Column\n    Returns the current date at the start of query evaluation as a :class:`DateType` column.\n    All calls of current_date within the same query return the same value.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        current date.\n    \n    Examples\n    --------\n    >>> df = spark.range(1)\n    >>> df.select(current_date()).show() # doctest: +SKIP\n    +--------------+\n    |current_date()|\n    +--------------+\n    |    2022-08-26|\n    +--------------+\n\n"
     ]
    }
   ],
   "source": [
    "help(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00dc4a82-6e4b-478a-a1e1-13e2832e8d93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n\nto_date(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n    using the optionally specified format. Specify formats according to `datetime pattern`_.\n    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n    is omitted. Equivalent to ``col.cast(\"date\")``.\n    \n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n    \n    .. versionadded:: 2.2.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    col : :class:`~pyspark.sql.Column` or str\n        input column of values to convert.\n    format: str, optional\n        format to use to convert date values.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        date value as :class:`pyspark.sql.types.DateType` type.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_date(df.t).alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n    \n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n\n"
     ]
    }
   ],
   "source": [
    "help(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55cf1a47-fe59-4024-aefd-d54f6fef584b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_timestamp in module pyspark.sql.functions:\n\nto_timestamp(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n    using the optionally specified format. Specify formats according to `datetime pattern`_.\n    By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n    is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n    \n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n    \n    .. versionadded:: 2.2.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    col : :class:`~pyspark.sql.Column` or str\n        column values to convert.\n    format: str, optional\n        format to use to convert timestamp values.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        timestamp value as :class:`pyspark.sql.types.TimestampType` type.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n    \n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n\n"
     ]
    }
   ],
   "source": [
    "help(to_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dce72718-5841-4a92-bed6-691827e015a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12 - Date and Time Manipulation Functions using Spark Data Frames",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

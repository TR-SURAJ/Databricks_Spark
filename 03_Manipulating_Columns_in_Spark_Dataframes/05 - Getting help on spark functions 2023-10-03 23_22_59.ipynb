{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d51b55d-7c6d-4b16-b65c-4319af2d6972",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb0e8d1-16f6-4192-b1d8-a4365879b3db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n\ndate_format(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n    Converts a date/timestamp/string to a value of string in the format specified by the date\n    format given by the second argument.\n    \n    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n    pattern letters of `datetime pattern`_. can be used.\n    \n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n    \n    .. versionadded:: 1.5.0\n    \n    Notes\n    -----\n    Whenever possible, use specialized functions like `year`.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n    [Row(date='04/08/2015')]\n\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a23ae08-9684-4780-8a53-5622b01f1405",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function col in module pyspark.sql.functions:\n\ncol(col: str) -> pyspark.sql.column.Column\n    Returns a :class:`~pyspark.sql.Column` based on the given column name.\n    \n    Examples\n    --------\n    >>> col('x')\n    Column<'x'>\n    >>> column('x')\n    Column<'x'>\n    \n    .. versionadded:: 1.3\n\n"
     ]
    }
   ],
   "source": [
    "help(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f24c48ea-4bf7-4c6b-abaa-41d6b49bfef7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lit in module pyspark.sql.functions:\n\nlit(col: Any) -> pyspark.sql.column.Column\n    Creates a :class:`~pyspark.sql.Column` of literal value.\n    \n    .. versionadded:: 1.3.0\n    \n    Examples\n    --------\n    >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n    [Row(height=5, spark_user=True)]\n\n"
     ]
    }
   ],
   "source": [
    "help(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b50cb84-6f7c-499e-97ff-2d406d34a126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module pyspark.sql.functions:\n\nconcat(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n    Concatenates multiple input columns together into a single column.\n    The function works with strings, binary and compatible array columns.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat(df.s, df.d).alias('s')).collect()\n    [Row(s='abcd123')]\n    \n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n\n"
     ]
    }
   ],
   "source": [
    "help(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c1595f-f7b3-443e-840e-549480b6f81c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f984116a-80f3-41d8-a296-d9ef5618db15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+\n|     a|     b|  c|\n+------+------+---+\n|[1, 2]|[3, 4]|[5]|\n|[1, 2]|  null|[3]|\n+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47eff4b8-3a0f-441c-aa5a-83484a48ba3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|            arr|\n+---------------+\n|[1, 2, 3, 4, 5]|\n|           null|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(concat(df.a, df.b, df.c).alias(\"arr\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a122998d-8a6f-49e7-b3e8-8bb3a06574de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat_ws in module pyspark.sql.functions:\n\nconcat_ws(sep: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n    Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n    [Row(s='abcd-123')]\n\n"
     ]
    }
   ],
   "source": [
    "help(concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eefae23f-2e2a-49a0-a711-deeec1ae2dc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05 - Getting help on spark functions 2023-10-03 23:22:59",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f4dca1-0154-4f27-831c-f8920ceade91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a07801e-bb1e-41fb-bafa-68b646f348a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [\n",
    "    (\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "    (\"2016-03-29\", \"2016-03-29 11:23:00.234\"),\n",
    "    (\"2018-04-20\", \"2018-04-20 12:34:00.543\"),\n",
    "    (\"2019-05-12\", \"2019-05-12 13:21:00.567\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3786b6ae-c544-4ab9-a3f7-02cbfbe75c90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datedf = spark.createDataFrame(datetimes, schema = 'date STRING, time STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a1938f-5a4b-49e0-b626-ce70f7d8004b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n|date      |time                   |\n+----------+-----------------------+\n|2014-02-28|2014-02-28 10:00:00.123|\n|2016-03-29|2016-03-29 11:23:00.234|\n|2018-04-20|2018-04-20 12:34:00.543|\n|2019-05-12|2019-05-12 13:21:00.567|\n+----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbff716b-b6e7-4c44-8377-8be2a719cb52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n\ndate_format(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n    Converts a date/timestamp/string to a value of string in the format specified by the date\n    format given by the second argument.\n    \n    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n    pattern letters of `datetime pattern`_. can be used.\n    \n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Notes\n    -----\n    Whenever possible, use specialized functions like `year`.\n    \n    Parameters\n    ----------\n    date : :class:`~pyspark.sql.Column` or str\n        input column of values to format.\n    format: str\n        format to use to represent datetime values.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        string value representing formatted datetime.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n    [Row(date='04/08/2015')]\n\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b07348ad-2f9e-4d3a-acbe-0c43e0c06612",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Get the year and month from both date and time columns using yyyyMM format. Also make sure that the data type is converted to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac58ba68-ca87-44aa-844b-4badf3f97642",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n|date      |time                   |date_ym|time_ym|\n+----------+-----------------------+-------+-------+\n|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n|2016-03-29|2016-03-29 11:23:00.234|201603 |201603 |\n|2018-04-20|2018-04-20 12:34:00.543|201804 |201804 |\n|2019-05-12|2019-05-12 13:21:00.567|201905 |201905 |\n+----------+-----------------------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"date_ym\", date_format(\"date\", \"yyyyMM\")). \\\n",
    "    withColumn(\"time_ym\", date_format(\"time\", \"yyyyMM\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1ee838d-01ad-48f5-b490-4ae1affcda8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- yyyy : year\n",
    "- MM   : month\n",
    "- dd   : day of the month\n",
    "- DD   : Julian day (day of the year)\n",
    "- HH   : 24 hour format\n",
    "- hh   : 12 hour format\n",
    "- mm   : minute\n",
    "- ss   : second\n",
    "- SSS  : milli second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5c7512-ba46-4d0d-9b36-d7c1cbbdb87a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n|date      |time                   |date_ym|time_ym|\n+----------+-----------------------+-------+-------+\n|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n|2016-03-29|2016-03-29 11:23:00.234|201603 |201603 |\n|2018-04-20|2018-04-20 12:34:00.543|201804 |201804 |\n|2019-05-12|2019-05-12 13:21:00.567|201905 |201905 |\n+----------+-----------------------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"date_ym\", date_format(\"date\", \"yyyyMM\").cast('int')). \\\n",
    "    withColumn(\"time_ym\", date_format(\"time\", \"yyyyMM\").cast('int')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2dc810-673a-4ff2-b6cb-a82d8383c13c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------------+--------------+\n|date      |time                   |date_dt       |time_ts       |\n+----------+-----------------------+--------------+--------------+\n|2014-02-28|2014-02-28 10:00:00.123|20140228000000|20140228100000|\n|2016-03-29|2016-03-29 11:23:00.234|20160329000000|20160329112300|\n|2018-04-20|2018-04-20 12:34:00.543|20180420000000|20180420123400|\n|2019-05-12|2019-05-12 13:21:00.567|20190512000000|20190512132100|\n+----------+-----------------------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"date_dt\", date_format(\"date\", \"yyyyMMddHHmmss\")). \\\n",
    "    withColumn(\"time_ts\", date_format(\"time\", \"yyyyMMddHHmmss\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1ca388-ffca-4e3e-806d-af7ff69f7fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n|date      |time                   |date_yd|time_yd|\n+----------+-----------------------+-------+-------+\n|2014-02-28|2014-02-28 10:00:00.123|2014059|2014059|\n|2016-03-29|2016-03-29 11:23:00.234|2016089|2016089|\n|2018-04-20|2018-04-20 12:34:00.543|2018110|2018110|\n|2019-05-12|2019-05-12 13:21:00.567|2019132|2019132|\n+----------+-----------------------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"date_yd\", date_format(\"date\", \"yyyyDDD\")). \\\n",
    "    withColumn(\"time_yd\", date_format(\"time\", \"yyyyDDD\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0078899-293f-45a1-9b31-4e066b13aaac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------------+\n|date      |time                   |date_yd          |\n+----------+-----------------------+-----------------+\n|2014-02-28|2014-02-28 10:00:00.123|February 28, 2014|\n|2016-03-29|2016-03-29 11:23:00.234|March 29, 2016   |\n|2018-04-20|2018-04-20 12:34:00.543|April 20, 2018   |\n|2019-05-12|2019-05-12 13:21:00.567|May 12, 2019     |\n+----------+-----------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "datedf. \\\n",
    "    withColumn(\"date_yd\", date_format(\"date\", \"MMMM d, yyyy\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e815b3-bc67-4958-95de-c82267d43d56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+\n|date      |time                   |date_abbr|\n+----------+-----------------------+---------+\n|2014-02-28|2014-02-28 10:00:00.123|Fri      |\n|2016-03-29|2016-03-29 11:23:00.234|Tue      |\n|2018-04-20|2018-04-20 12:34:00.543|Fri      |\n|2019-05-12|2019-05-12 13:21:00.567|Sun      |\n+----------+-----------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# name of weekday\n",
    "datedf. \\\n",
    "    withColumn(\"date_abbr\", date_format(\"date\", \"EE\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97e0cf8-e5fc-4a09-86c9-ae77358e8f02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+\n|date      |time                   |date_abbr|\n+----------+-----------------------+---------+\n|2014-02-28|2014-02-28 10:00:00.123|Friday   |\n|2016-03-29|2016-03-29 11:23:00.234|Tuesday  |\n|2018-04-20|2018-04-20 12:34:00.543|Friday   |\n|2019-05-12|2019-05-12 13:21:00.567|Sunday   |\n+----------+-----------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# full name of weekday\n",
    "datedf. \\\n",
    "    withColumn(\"date_abbr\", date_format(\"date\", \"EEEE\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f1f9a8a-8c47-4cf7-bf52-454bafb8109d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "17 - Using date_format function on Spark Data Frames",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

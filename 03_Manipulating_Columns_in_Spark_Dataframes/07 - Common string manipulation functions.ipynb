{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404a88c8-6722-4b7e-8bfc-3fe3f6e65a51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_list = [(1,\"bob\",\"jackson\",1500.0,\"AUSTRALIA\",\"+61 9890989789\",\"345 23 5645\"),\n",
    "            (2,\"hill\",\"martin\",750.0,\"UK\",\"+44 3465789709\",\"390 45 7598\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3fba626-c9d9-44b8-8dc6-aa8475d4061b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp = spark.createDataFrame(employee_list, schema = \"\"\"employee_id INT, first_name STRING, last_name STRING, salary FLOAT, nationality STRING,\n",
    "                            phone_number STRING, ssn STRING \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a109a8d8-f178-4c98-9587-60d23c2e99fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd3675c-58df-4db9-9278-46b6ee61b7c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|\n+-----------+----------+---------+------+-----------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec8c488-0b22-41a5-91f8-25e8f1107c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+----------+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn| full_name|\n+-----------+----------+---------+------+-----------+--------------+-----------+----------+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|bobjackson|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|hillmartin|\n+-----------+----------+---------+------+-----------+--------------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.withColumn(\"full_name\", concat(\"first_name\",\"last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770ac037-e297-45b9-918a-8a031dacc29f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat_ws in module pyspark.sql.functions:\n\nconcat_ws(sep: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n    Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n    [Row(s='abcd-123')]\n\n"
     ]
    }
   ],
   "source": [
    "help(concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e75dba-9c66-4258-bf37-8f196ba6fdea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------+--------------+-----------+-----------+\n|employee_id|first_name|last_name|salary|nationality|  phone_number|        ssn|  full_name|\n+-----------+----------+---------+------+-----------+--------------+-----------+-----------+\n|          1|       bob|  jackson|1500.0|  AUSTRALIA|+61 9890989789|345 23 5645|bob-jackson|\n|          2|      hill|   martin| 750.0|         UK|+44 3465789709|390 45 7598|hill-martin|\n+-----------+----------+---------+------+-----------+--------------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp.withColumn(\"full_name\", concat_ws(\"-\",\"first_name\",\"last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6307e7e-c792-4a8e-8927-87c3b8aed121",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "\u001B[0;32m<command-3025750883095284>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"full_name\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconcat_ws\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\", \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"first_name\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"last_name\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36mconcat_ws\u001B[0;34m(sep, *cols)\u001B[0m\n",
       "\u001B[1;32m   2974\u001B[0m     \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   2975\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0msc\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 2976\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_invoke_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"concat_ws\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_to_java_column\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   2977\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36m_invoke_function\u001B[0;34m(name, *args)\u001B[0m\n",
       "\u001B[1;32m     85\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     86\u001B[0m     \u001B[0mjf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_jvm_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 87\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     88\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1312\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1313\u001B[0;31m         \u001B[0margs_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   1314\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m         \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCALL_COMMAND_NAME\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_build_args\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1275\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1276\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1277\u001B[0;31m             \u001B[0;34m(\u001B[0m\u001B[0mnew_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   1278\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1279\u001B[0m             \u001B[0mnew_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_get_args\u001B[0;34m(self, args)\u001B[0m\n",
       "\u001B[1;32m   1262\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mconverter\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1263\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcan_convert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1264\u001B[0;31m                         \u001B[0mtemp_arg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   1265\u001B[0m                         \u001B[0mtemp_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1266\u001B[0m                         \u001B[0mnew_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_collections.py\u001B[0m in \u001B[0;36mconvert\u001B[0;34m(self, object, gateway_client)\u001B[0m\n",
       "\u001B[1;32m    508\u001B[0m         \u001B[0mArrayList\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mJavaClass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"java.util.ArrayList\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    509\u001B[0m         \u001B[0mjava_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mArrayList\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 510\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0melement\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mobject\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    511\u001B[0m             \u001B[0mjava_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    512\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjava_list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    565\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    566\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 567\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Column is not iterable\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    568\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    569\u001B[0m     \u001B[0;31m# string methods\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: Column is not iterable"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3025750883095284>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"full_name\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconcat_ws\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\", \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"first_name\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"last_name\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36mconcat_ws\u001B[0;34m(sep, *cols)\u001B[0m\n\u001B[1;32m   2974\u001B[0m     \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2975\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0msc\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2976\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_invoke_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"concat_ws\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_to_java_column\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2977\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36m_invoke_function\u001B[0;34m(name, *args)\u001B[0m\n\u001B[1;32m     85\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m     \u001B[0mjf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_jvm_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1312\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1313\u001B[0;31m         \u001B[0margs_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1314\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1315\u001B[0m         \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCALL_COMMAND_NAME\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_build_args\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1275\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1276\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1277\u001B[0;31m             \u001B[0;34m(\u001B[0m\u001B[0mnew_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1278\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1279\u001B[0m             \u001B[0mnew_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_get_args\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m   1262\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mconverter\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1263\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcan_convert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1264\u001B[0;31m                         \u001B[0mtemp_arg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1265\u001B[0m                         \u001B[0mtemp_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1266\u001B[0m                         \u001B[0mnew_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_collections.py\u001B[0m in \u001B[0;36mconvert\u001B[0;34m(self, object, gateway_client)\u001B[0m\n\u001B[1;32m    508\u001B[0m         \u001B[0mArrayList\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mJavaClass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"java.util.ArrayList\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    509\u001B[0m         \u001B[0mjava_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mArrayList\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 510\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0melement\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mobject\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    511\u001B[0m             \u001B[0mjava_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    512\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjava_list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    566\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 567\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Column is not iterable\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    568\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    569\u001B[0m     \u001B[0;31m# string methods\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: Column is not iterable",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: Column is not iterable",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp.withColumn(\"full_name\", concat_ws(lit(\", \"),\"first_name\",\"last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667b1784-bd82-458d-b878-6638bf8ca510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "address = [\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"address\": \"9 debs parkway\",\n",
    "        \"city\": \"new york city\",\n",
    "        \"state\": \"new york\",\n",
    "        \"country\": \"US\",\n",
    "        \"postal_code\": \"10090\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"address\": \"3645 dayton hill\",\n",
    "        \"city\": \"newton\",\n",
    "        \"state\": \"massachusetts\",\n",
    "        \"country\": \"US\",\n",
    "        \"postal_code\": \"02162\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"address\": \"1551 6th plaza\",\n",
    "        \"city\": \"modesto\",\n",
    "        \"state\": \"california\",\n",
    "        \"country\": \"US\",\n",
    "        \"postal_code\": \"95354\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 13,\n",
    "        \"address\": \"7849 ohio drive\",\n",
    "        \"city\": \"springfield\",\n",
    "        \"state\": \"missouri\",\n",
    "        \"country\": \"US\",\n",
    "        \"postal_code\": \"10120\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da673c9e-ae49-491a-ace1-239a2f4c9a20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "addressdf = spark.createDataFrame(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0043015-ddb6-48f0-b95c-52a1802b5d82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------+---+-----------+-------------+\n|         address|         city|country| id|postal_code|        state|\n+----------------+-------------+-------+---+-----------+-------------+\n|  9 debs parkway|new york city|     US| 10|      10090|     new york|\n|3645 dayton hill|       newton|     US| 11|      02162|massachusetts|\n|  1551 6th plaza|      modesto|     US| 12|      95354|   california|\n| 7849 ohio drive|  springfield|     US| 13|      10120|     missouri|\n+----------------+-------------+-------+---+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "addressdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92952e20-44fd-4a20-9a11-5ba33d3b412d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+\n|id |full_address                                      |\n+---+--------------------------------------------------+\n|10 |9 debs parkway, new york city, new york, US, 10090|\n|11 |3645 dayton hill, newton, massachusetts, US, 02162|\n|12 |1551 6th plaza, modesto, california, US, 95354    |\n|13 |7849 ohio drive, springfield, missouri, US, 10120 |\n+---+--------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "addressdf.select('id', concat_ws(', ', 'address','city','state','country','postal_code').alias('full_address')).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b07a4f1-0f01-4539-a8e2-53c293ff40c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61cb3c29-e39d-4b8f-bf52-88615d6564a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function initcap in module pyspark.sql.functions:\n\ninitcap(col: 'ColumnOrName') -> pyspark.sql.column.Column\n    Translate the first letter of each word to upper case in the sentence.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n    [Row(v='Ab Cd')]\n\n"
     ]
    }
   ],
   "source": [
    "help(initcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79fb7c9-354c-44d5-b0f8-6ccdcd6579f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function length in module pyspark.sql.functions:\n\nlength(col: 'ColumnOrName') -> pyspark.sql.column.Column\n    Computes the character length of string data or number of bytes of binary data.\n    The length of character data includes the trailing spaces. The length of binary data\n    includes binary zeros.\n    \n    .. versionadded:: 1.5.0\n    \n    Examples\n    --------\n    >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n    [Row(length=4)]\n\n"
     ]
    }
   ],
   "source": [
    "help(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586080bc-5f85-4e13-818c-ca219829df07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+-------------+-------------+------------+\n| id|        state|  state_upper|  state_lower|state_initcap|state_length|\n+---+-------------+-------------+-------------+-------------+------------+\n| 10|     new york|     NEW YORK|     new york|     New York|           8|\n| 11|massachusetts|MASSACHUSETTS|massachusetts|Massachusetts|          13|\n| 12|   california|   CALIFORNIA|   california|   California|          10|\n| 13|     missouri|     MISSOURI|     missouri|     Missouri|           8|\n+---+-------------+-------------+-------------+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "addressdf.select('id','state'). \\\n",
    "    withColumn(\"state_upper\", upper('state')).\\\n",
    "    withColumn(\"state_lower\", lower('state')). \\\n",
    "    withColumn(\"state_initcap\", initcap(\"state\")).\\\n",
    "    withColumn(\"state_length\", length(\"state\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8964afe3-ed68-4045-a3fe-11ca6fd2f671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07 - Common string manipulation functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

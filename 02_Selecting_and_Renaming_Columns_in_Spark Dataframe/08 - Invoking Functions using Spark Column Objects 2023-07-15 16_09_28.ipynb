{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd3fc75-ed3d-4354-b170-75b6a45d63d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n|  1|    Corrie|Van den Oord| cvandenoor@etsy.com|{+91 8645879087, ...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n|  2|      John|        Cena|       john@cena.com|{+91 9886879087, ...| [3, 4]|       true|      900.0|   2022-05-15|2024-03-15 01:16:00|\n|  3|     James|        Bond|      james@bond.com|{+91 3245879087, ...|    [2]|      false|      750.6|   2023-01-12|2018-05-05 05:17:02|\n|  4|    Robert|      Dowrey|   robert@dowrey.com|                null|     []|       true|        NaN|         null|2019-04-03 08:14:08|\n|  5|     Chris|  Hemmsworth|chris@hemmsworth.com|{+91 9085879087, ...|     []|      false|        NaN|         null|2019-04-03 08:14:08|\n+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [('id', 'bigint'),\n ('first_name', 'string'),\n ('last_name', 'string'),\n ('email', 'string'),\n ('phone_numbers', 'struct<mobile:string,home:string>'),\n ('courses', 'array<bigint>'),\n ('is_customer', 'boolean'),\n ('amount_paid', 'double'),\n ('customer_from', 'date'),\n ('last_updated_ts', 'timestamp')]"
     ]
    }
   ],
   "source": [
    "%run \"/Users/surajthallapalli@outlook.com/02 Selecting and Renaming Columns in Spark Dataframe/Creating Spark Dataframe 2023-07-11 20:32:19\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c611d64e-f1f2-4794-bc3d-943c88e937ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* Concatenate `first_name` and `last_name` to generate `full_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a187e2a6-8b6c-4adc-877f-031518a26055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b28141c-d7c0-478a-b540-f6ddc53fdeaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module pyspark.sql.functions:\n\nconcat(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n    Concatenates multiple input columns together into a single column.\n    The function works with strings, numeric, binary and compatible array columns.\n    \n    .. versionadded:: 1.5.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    cols : :class:`~pyspark.sql.Column` or str\n        target column or columns to work on.\n    \n    Returns\n    -------\n    :class:`~pyspark.sql.Column`\n        concatenated values. Type of the `Column` depends on input columns' type.\n    \n    See Also\n    --------\n    :meth:`pyspark.sql.functions.array_join` : to concatenate string columns with delimiter\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df = df.select(concat(df.s, df.d).alias('s'))\n    >>> df.collect()\n    [Row(s='abcd123')]\n    >>> df\n    DataFrame[s: string]\n    \n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n    >>> df = df.select(concat(df.a, df.b, df.c).alias(\"arr\"))\n    >>> df.collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n    >>> df\n    DataFrame[arr: array<bigint>]\n\n"
     ]
    }
   ],
   "source": [
    "help(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4ca7d6-9b03-4f03-b4b8-4233fb6d0692",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "full_name_col = concat(col('first_name'), lit(','), col('last_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6ae8c8-2811-409f-abb0-51ec60e82780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: Column<'concat(first_name, ,, last_name)'>"
     ]
    }
   ],
   "source": [
    "full_name_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de00c71c-725f-4207-b1d2-f240be959890",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "full_name_alias = full_name_col.alias('full_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89152420-3e3c-4cff-8d24-e46f4f6c940e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: Column<'concat(first_name, ,, last_name) AS full_name'>"
     ]
    }
   ],
   "source": [
    "full_name_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84a63c0-a943-47b5-8b19-13ff328d4d2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n| id|          full_name|\n+---+-------------------+\n|  1|Corrie,Van den Oord|\n|  2|          John,Cena|\n|  3|         James,Bond|\n|  4|      Robert,Dowrey|\n|  5|   Chris,Hemmsworth|\n+---+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "users_df.select('id', full_name_alias).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b3c2c76-d1ac-4598-807b-a6a98085b5bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* Convert data type of customer_from date to numeric type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39bfb26-c0a0-4e0b-9870-4e9fd936a0c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ae8395-85f6-4058-bb40-e49f03554f26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: Column<'CAST(date_format(customer_from, yyyymmdd) AS INT) AS customer_from'>"
     ]
    }
   ],
   "source": [
    "date_format('customer_from','yyyymmdd').cast('int').alias('customer_from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fa7880-4ffa-41e0-8276-d5fbee9b8cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_from_alias = date_format('customer_from','yyyymmdd').cast('int').alias('customer_from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1257bc-b5e1-45b1-af4c-03ee063bcb67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n| id|customer_from|\n+---+-------------+\n|  1|     20210015|\n|  2|     20220015|\n|  3|     20230012|\n|  4|         null|\n|  5|         null|\n+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "users_df.select('id',customer_from_alias).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ed5661-95cf-4022-95f6-490d6648404e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08 - Invoking Functions using Spark Column Objects 2023-07-15 16:09:28",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

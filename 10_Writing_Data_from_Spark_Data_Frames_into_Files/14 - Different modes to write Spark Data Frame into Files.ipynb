{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a9bab9-a1fe-4a70-a266-72d0ca3e8027",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "courses = [\n",
    "    {   \n",
    "        'course_id': 1,\n",
    "        'course_name': \"Python Bootcamp\",\n",
    "        'suitable_for': 'Beginner',\n",
    "        'enrollment': 1100093,\n",
    "        'stars': 4.6,\n",
    "        'number_of_ratings': 218066\n",
    "    },\n",
    "    {   \n",
    "        'course_id': 2,\n",
    "        'course_name': \"angular - the complete guide\",\n",
    "        'suitable_for': 'Intermediate',\n",
    "        'enrollment': 34567,\n",
    "        'stars': 4.5,\n",
    "        'number_of_ratings': 347912\n",
    "    },\n",
    "    {\n",
    "        'course_id': 3,\n",
    "        'course_name': \"Java In-Depth\",\n",
    "        'suitable_for': 'Adavanced',\n",
    "        'enrollment': 2345321,\n",
    "        'stars': 4.8,\n",
    "        'number_of_ratings': 23789\n",
    "    },\n",
    "    {\n",
    "        'course_id': 4,\n",
    "        'course_name': \"C++ Beginner guide\",\n",
    "        'suitable_for': 'Beginner',\n",
    "        'enrollment': 32145,\n",
    "        'stars': 4.2,\n",
    "        'number_of_ratings': 5678\n",
    "    },\n",
    "    {\n",
    "        'course_id': 5,\n",
    "        'course_name': \"Data Science Practical approach\",\n",
    "        'suitable_for': 'Intermediate',\n",
    "        'enrollment': 67897,\n",
    "        'stars': 4.6,\n",
    "        'number_of_ratings': 267576\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e28dd10-4a09-4416-99e6-b52790fe2663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd2ec29-b3e5-4eae-9bde-7517406e7dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df = spark.createDataFrame([Row(**course) for course in courses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b9d330-1270-4e64-83e4-11468db439ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf0c7e8-6f0f-4c4a-8d2d-85aa05ae5625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method mode in module pyspark.sql.readwriter:\n\nmode(saveMode: Optional[str]) -> 'DataFrameWriter' method of pyspark.sql.readwriter.DataFrameWriter instance\n    Specifies the behavior when data or table already exists.\n    \n    Options include:\n    \n    * `append`: Append contents of this :class:`DataFrame` to existing data.\n    * `overwrite`: Overwrite existing data.\n    * `error` or `errorifexists`: Throw an exception if data already exists.\n    * `ignore`: Silently ignore this operation if data already exists.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Examples\n    --------\n    Raise an error when writing to an existing path.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 80, \"name\": \"Xinrong Meng\"}]\n    ...     ).write.mode(\"error\").format(\"parquet\").save(d) # doctest: +SKIP\n    Traceback (most recent call last):\n        ...\n    ...AnalysisException: ...\n    \n    Write a Parquet file back with various options, and read it back.\n    \n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Overwrite the path with a new Parquet file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n    ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n    ...\n    ...     # Append another DataFrame into the Parquet file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 120, \"name\": \"Takuya Ueshin\"}]\n    ...     ).write.mode(\"append\").format(\"parquet\").save(d)\n    ...\n    ...     # Append another DataFrame into the Parquet file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 140, \"name\": \"Haejoon Lee\"}]\n    ...     ).write.mode(\"ignore\").format(\"parquet\").save(d)\n    ...\n    ...     # Read the Parquet file as a DataFrame.\n    ...     spark.read.parquet(d).show()\n    +---+-------------+\n    |age|         name|\n    +---+-------------+\n    |120|Takuya Ueshin|\n    |100| Hyukjin Kwon|\n    +---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "help(course_df.write.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156f08ea-7826-44f4-91c7-d7b6ace6d4bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Different ways mode can be specified while writing data frame into files. *file_format* can be any valid out of box format such as text, csv, json, parquet. orc\n",
    "\n",
    "- course_df.write.mode(saveMode).file_format(path_to_folder)\n",
    "- course_df.write.file_format(path_to_folder,mode=saveMode)\n",
    "- course_df.write.mode(saveMode).format('file_format').save(path_to_folder)\n",
    "- course_df.write.format('file_format').save(path_to_folder, mode=saveMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f389960-a53b-4f76-8fe5-28210b740481",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Understand default behavior\n",
    "  - Fails if folder exists\n",
    "  - Creates folder and then adds files to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa2a4de-435a-4072-89bd-82659a19e99c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.rm(f'/user/{username}/course', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aebe8d6-944a-4b1d-b2ae-c205d23b0e9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    parquet(\n",
    "        f'/user/{username}/course'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddff7a11-dae1-4ffe-bc7c-0ddee02cfc57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/user/root/course/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1698123636000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_6160619855608807065', name='_committed_6160619855608807065', size=123, modificationTime=1698123636000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_6160619855608807065', name='_started_6160619855608807065', size=0, modificationTime=1698123635000),\n",
       " FileInfo(path='dbfs:/user/root/course/part-00000-tid-6160619855608807065-c6c000b0-c19f-4aa7-9038-b055c81d5507-25-1.c000.snappy.parquet', name='part-00000-tid-6160619855608807065-c6c000b0-c19f-4aa7-9038-b055c81d5507-25-1.c000.snappy.parquet', size=2031, modificationTime=1698123636000)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c12752-a550-4b28-b3c8-8ed1d65af698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2792568061432263>, line 5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# fails as mode is error or errorIfExists by default\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43mcourse_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoalesce\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n",
       "\u001B[0;32m----> 5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/user/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43musername\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/course\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1721\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n",
       "\u001B[1;32m   1719\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n",
       "\u001B[1;32m   1720\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n",
       "\u001B[0;32m-> 1721\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_ALREADY_EXISTS] Path dbfs:/user/root/course already exists. Set mode as \"overwrite\" to overwrite the existing path."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2792568061432263>, line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# fails as mode is error or errorIfExists by default\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[43mcourse_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoalesce\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m\\\u001B[49m\n\u001B[0;32m----> 5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/user/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43musername\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/course\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1721\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m   1719\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n\u001B[1;32m   1720\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n\u001B[0;32m-> 1721\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [PATH_ALREADY_EXISTS] Path dbfs:/user/root/course already exists. Set mode as \"overwrite\" to overwrite the existing path.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [PATH_ALREADY_EXISTS] Path dbfs:/user/root/course already exists. Set mode as \"overwrite\" to overwrite the existing path.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fails as mode is error or errorIfExists by default\n",
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    parquet(\n",
    "        f'/user/{username}/course'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e9c37d-13db-471b-813a-739ef19e0e37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/course').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97511f5e-14b9-4b18-abd9-b75e8b35931d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    mode('overwrite').\\\n",
    "    parquet(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b64b72-5ab1-41fd-84a3-a736368dd84d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    parquet(f'/user/{username}/course', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec5a665-9342-42a1-83c1-cc3cd54a7bf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    format('parquet').\\\n",
    "    save(f'/user/{username}/course',mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ac8220-63c9-4946-8fe7-16f540434d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    mode('append').\\\n",
    "    parquet(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c527fa5-3928-4a3c-b628-08854fdce15a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/user/root/course/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1698124256000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_2775427910264764123', name='_committed_2775427910264764123', size=232, modificationTime=1698123976000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_4562339149116089518', name='_committed_4562339149116089518', size=221, modificationTime=1698124196000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_6116636353908954032', name='_committed_6116636353908954032', size=221, modificationTime=1698124112000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_6160619855608807065', name='_committed_6160619855608807065', size=123, modificationTime=1698123636000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_7233770919239630562', name='_committed_7233770919239630562', size=123, modificationTime=1698124255000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_2775427910264764123', name='_started_2775427910264764123', size=0, modificationTime=1698123975000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_4562339149116089518', name='_started_4562339149116089518', size=0, modificationTime=1698124196000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_6116636353908954032', name='_started_6116636353908954032', size=0, modificationTime=1698124111000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_6160619855608807065', name='_started_6160619855608807065', size=0, modificationTime=1698123635000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_7233770919239630562', name='_started_7233770919239630562', size=0, modificationTime=1698124255000),\n",
       " FileInfo(path='dbfs:/user/root/course/part-00000-tid-4562339149116089518-19dafd82-8877-41c8-bd8c-47e6b203a9a9-31-1.c000.snappy.parquet', name='part-00000-tid-4562339149116089518-19dafd82-8877-41c8-bd8c-47e6b203a9a9-31-1.c000.snappy.parquet', size=2031, modificationTime=1698124196000),\n",
       " FileInfo(path='dbfs:/user/root/course/part-00000-tid-7233770919239630562-dcf64b11-e4db-414c-8b9f-b1985cdd34fb-32-1.c000.snappy.parquet', name='part-00000-tid-7233770919239630562-dcf64b11-e4db-414c-8b9f-b1985cdd34fb-32-1.c000.snappy.parquet', size=2031, modificationTime=1698124255000)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac2a1ed-9e43-4fd2-85fd-5c2a96b7d469",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/course').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39094a8a-4b96-4231-8b50-7bf6df157086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "14 - Different modes to write Spark Data Frame into Files",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749bd580-94a9-40a1-bb89-907c9fd206fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "courses = [\n",
    "    {   \n",
    "        'course_id': 1,\n",
    "        'course_name': \"Python Bootcamp\",\n",
    "        'suitable_for': 'Beginner',\n",
    "        'enrollment': 1100093,\n",
    "        'stars': 4.6,\n",
    "        'number_of_ratings': 218066\n",
    "    },\n",
    "    {   \n",
    "        'course_id': 2,\n",
    "        'course_name': \"angular - the complete guide\",\n",
    "        'suitable_for': 'Intermediate',\n",
    "        'enrollment': 34567,\n",
    "        'stars': 4.5,\n",
    "        'number_of_ratings': 347912\n",
    "    },\n",
    "    {\n",
    "        'course_id': 3,\n",
    "        'course_name': \"Java In-Depth\",\n",
    "        'suitable_for': 'Adavanced',\n",
    "        'enrollment': 2345321,\n",
    "        'stars': 4.8,\n",
    "        'number_of_ratings': 23789\n",
    "    },\n",
    "    {\n",
    "        'course_id': 4,\n",
    "        'course_name': \"C++ Beginner guide\",\n",
    "        'suitable_for': 'Beginner',\n",
    "        'enrollment': 32145,\n",
    "        'stars': 4.2,\n",
    "        'number_of_ratings': 5678\n",
    "    },\n",
    "    {\n",
    "        'course_id': 5,\n",
    "        'course_name': \"Data Science Practical approach\",\n",
    "        'suitable_for': 'Intermediate',\n",
    "        'enrollment': 67897,\n",
    "        'stars': 4.6,\n",
    "        'number_of_ratings': 267576\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c344e9-17a7-4d35-9d74-8d3dc25f86ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8af2eed-7174-4928-8af4-64071f857856",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df = spark.createDataFrame([Row(**course) for course in courses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a87fc8-68e5-4eeb-a1b9-82113aa83a53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19655cf4-1d08-4fc6-a625-2fbe1619b1e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.rm(f'/user/{username}/course', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c96a08-91eb-4415-8921-fea69465c747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2792568061432238>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/user/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43musername\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/course\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o773.ls.\n",
       ": java.io.FileNotFoundException: No such file or directory /user/root/course\n",
       "\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:138)\n",
       "\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n",
       "\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:118)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:639)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:639)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:639)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:639)\n",
       "\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:118)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-2792568061432238>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/user/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43musername\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/course\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o773.ls.\n: java.io.FileNotFoundException: No such file or directory /user/root/course\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:138)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:118)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:639)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:639)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:639)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:639)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:118)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "java.io.FileNotFoundException: No such file or directory /user/root/course",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.ls(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a456c3-f257-4c85-ad8e-0e42a9d9d5f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# By default parquet files are compressed using snappy\n",
    "\n",
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    parquet(\n",
    "        f'/user/{username}/course',\n",
    "        mode='overwrite'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba9ba7b-e958-46e8-9255-ac74d49f78bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/user/root/course/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1698122142000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_5029909675064154924', name='_committed_5029909675064154924', size=123, modificationTime=1698122142000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_5029909675064154924', name='_started_5029909675064154924', size=0, modificationTime=1698122142000),\n",
       " FileInfo(path='dbfs:/user/root/course/part-00000-tid-5029909675064154924-cd7ec5b0-95b4-4215-80f0-a367c41ebd01-19-1.c000.snappy.parquet', name='part-00000-tid-5029909675064154924-cd7ec5b0-95b4-4215-80f0-a367c41ebd01-19-1.c000.snappy.parquet', size=2031, modificationTime=1698122142000)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f1017b-f811-4aed-8e06-c07c061397ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+----------+-----+-----------------+\n|course_id|         course_name|suitable_for|enrollment|stars|number_of_ratings|\n+---------+--------------------+------------+----------+-----+-----------------+\n|        1|     Python Bootcamp|    Beginner|   1100093|  4.6|           218066|\n|        2|angular - the com...|Intermediate|     34567|  4.5|           347912|\n|        3|       Java In-Depth|   Adavanced|   2345321|  4.8|            23789|\n|        4|  C++ Beginner guide|    Beginner|     32145|  4.2|             5678|\n|        5|Data Science Prac...|Intermediate|     67897|  4.6|           267576|\n+---------+--------------------+------------+----------+-----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/course').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3d09c0-e185-496b-88a9-97d59ebca6a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.rm(f'/user/{username}/course', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3b60bc-2958-4f52-b9dc-fa40662e1f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parquet in module pyspark.sql.readwriter:\n\nparquet(path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    partitionBy : str or list, optional\n        names of partitioning columns\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a Parquet file and read it back.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a Parquet file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n    ...     ).write.parquet(d, mode=\"overwrite\")\n    ...\n    ...     # Read the Parquet file as a DataFrame.\n    ...     spark.read.format(\"parquet\").load(d).show()\n    +---+------------+\n    |age|        name|\n    +---+------------+\n    |100|Hyukjin Kwon|\n    +---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "help(course_df.write.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811db14b-8dce-4c1f-aa39-4b0ca04eba04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'snappy'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.parquet.compression.codec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708ff45d-4c35-41a6-b55e-b936e0668d8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write parquet files without compression. compression can be set to none or uncompressed\n",
    "\n",
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    format('parquet').\\\n",
    "    save(\n",
    "        f'/user/{username}/course',\n",
    "        mode='overwrite',\n",
    "        compression='none'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79307d5-5ede-4a69-bde6-0052df44549e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/user/root/course/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1698122489000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_6337364338659823487', name='_committed_6337364338659823487', size=116, modificationTime=1698122488000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_6337364338659823487', name='_started_6337364338659823487', size=0, modificationTime=1698122488000),\n",
       " FileInfo(path='dbfs:/user/root/course/part-00000-tid-6337364338659823487-8b0f8fe2-9de3-41ce-ae09-fb6d0f2233a7-22-1.c000.parquet', name='part-00000-tid-6337364338659823487-8b0f8fe2-9de3-41ce-ae09-fb6d0f2233a7-22-1.c000.parquet', size=2034, modificationTime=1698122488000)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d25624e-5179-4a0e-831b-f0dc3310a3a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    format('parquet').\\\n",
    "    save(\n",
    "        f'/user/{username}/course',\n",
    "        mode='overwrite',\n",
    "        compression='gzip'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf893fe-92c6-423a-abc3-e3f47e5d1de4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/user/root/course/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1698122562000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_6337364338659823487', name='_committed_6337364338659823487', size=116, modificationTime=1698122488000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_9110293867051764724', name='_committed_9110293867051764724', size=221, modificationTime=1698122562000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_6337364338659823487', name='_started_6337364338659823487', size=0, modificationTime=1698122488000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_9110293867051764724', name='_started_9110293867051764724', size=0, modificationTime=1698122560000),\n",
       " FileInfo(path='dbfs:/user/root/course/part-00000-tid-9110293867051764724-68e6fdf3-a256-4c80-931a-f7bdde096edc-23-1-c000.gz.parquet', name='part-00000-tid-9110293867051764724-68e6fdf3-a256-4c80-931a-f7bdde096edc-23-1-c000.gz.parquet', size=2430, modificationTime=1698122562000)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb2481b-027c-42de-a6e4-6eb45a10d102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.parquet.compression.codec','none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a4b5dc-7ad1-4333-ab63-d7d67351001d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_df.\\\n",
    "    coalesce(1).\\\n",
    "    write.\\\n",
    "    format('parquet').\\\n",
    "    save(\n",
    "        f'/user/{username}/course',\n",
    "        mode='overwrite'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333c87f0-8199-46fb-826d-03a0e0f7bd50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/user/root/course/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1698122627000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_2118053746300335729', name='_committed_2118053746300335729', size=210, modificationTime=1698122627000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_6337364338659823487', name='_committed_6337364338659823487', size=116, modificationTime=1698122488000),\n",
       " FileInfo(path='dbfs:/user/root/course/_committed_9110293867051764724', name='_committed_9110293867051764724', size=221, modificationTime=1698122562000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_2118053746300335729', name='_started_2118053746300335729', size=0, modificationTime=1698122627000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_6337364338659823487', name='_started_6337364338659823487', size=0, modificationTime=1698122488000),\n",
       " FileInfo(path='dbfs:/user/root/course/_started_9110293867051764724', name='_started_9110293867051764724', size=0, modificationTime=1698122560000),\n",
       " FileInfo(path='dbfs:/user/root/course/part-00000-tid-2118053746300335729-ccf5d4d7-b023-4f1b-866f-7b898d11cbfc-24-1.c000.parquet', name='part-00000-tid-2118053746300335729-ccf5d4d7-b023-4f1b-866f-7b898d11cbfc-24-1.c000.parquet', size=2034, modificationTime=1698122627000)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f'/user/{username}/course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86c7e7d6-4fa4-4fa1-8628-024cb2b51970",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13 - Compression while writing Spark Data Frames into parquet Files",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
